{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation - Combo Loss Model\n",
    "Evaluates the 3D U-Net trained with Combo Loss (Dice + Focal + Tversky) on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU available: {tf.config.list_physical_devices('GPU')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Define Loss Functions (needed for model loading)\n",
    "# =============================================================================\n",
    "\n",
    "# Class weights and parameters (must match training)\n",
    "CLASS_WEIGHTS = [0.1, 1.0, 30.0]\n",
    "TVERSKY_ALPHA = 0.3\n",
    "TVERSKY_BETA = 0.7\n",
    "FOCAL_GAMMA = 2.0\n",
    "LOSS_WEIGHTS = {'dice': 1.0, 'focal': 1.0, 'tversky': 1.0}\n",
    "\n",
    "\n",
    "def dice_coefficient_per_class(y_true, y_pred, class_idx, smooth=1e-6):\n",
    "    \"\"\"Dice coefficient for a single class.\"\"\"\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.cast(y_pred, tf.float32)\n",
    "    y_true_c = y_true[..., class_idx]\n",
    "    y_pred_c = y_pred[..., class_idx]\n",
    "    y_true_f = tf.keras.backend.flatten(y_true_c)\n",
    "    y_pred_f = tf.keras.backend.flatten(y_pred_c)\n",
    "    intersection = tf.keras.backend.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (tf.keras.backend.sum(y_true_f) + tf.keras.backend.sum(y_pred_f) + smooth)\n",
    "\n",
    "\n",
    "def weighted_dice_loss(y_true, y_pred):\n",
    "    \"\"\"Weighted dice loss.\"\"\"\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.cast(y_pred, tf.float32)\n",
    "    total_loss = 0.0\n",
    "    for class_idx, weight in enumerate(CLASS_WEIGHTS):\n",
    "        dice = dice_coefficient_per_class(y_true, y_pred, class_idx)\n",
    "        total_loss += weight * (1 - dice)\n",
    "    return total_loss / sum(CLASS_WEIGHTS)\n",
    "\n",
    "\n",
    "def focal_loss(y_true, y_pred, gamma=FOCAL_GAMMA):\n",
    "    \"\"\"Focal loss for handling class imbalance.\"\"\"\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.cast(y_pred, tf.float32)\n",
    "    y_pred = tf.clip_by_value(y_pred, 1e-7, 1 - 1e-7)\n",
    "    focal_weight = tf.pow(1 - y_pred, gamma)\n",
    "    ce = -y_true * tf.math.log(y_pred)\n",
    "    class_weights_tensor = tf.constant(CLASS_WEIGHTS, dtype=tf.float32)\n",
    "    class_weights_tensor = class_weights_tensor / tf.reduce_sum(class_weights_tensor)\n",
    "    focal = focal_weight * ce * class_weights_tensor\n",
    "    return tf.reduce_mean(tf.reduce_sum(focal, axis=-1))\n",
    "\n",
    "\n",
    "def tversky_loss(y_true, y_pred, alpha=TVERSKY_ALPHA, beta=TVERSKY_BETA, smooth=1e-6):\n",
    "    \"\"\"Tversky loss for controlling precision/recall tradeoff.\"\"\"\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.cast(y_pred, tf.float32)\n",
    "    total_loss = 0.0\n",
    "    for class_idx, weight in enumerate(CLASS_WEIGHTS):\n",
    "        y_true_c = y_true[..., class_idx]\n",
    "        y_pred_c = y_pred[..., class_idx]\n",
    "        y_true_f = tf.keras.backend.flatten(y_true_c)\n",
    "        y_pred_f = tf.keras.backend.flatten(y_pred_c)\n",
    "        tp = tf.keras.backend.sum(y_true_f * y_pred_f)\n",
    "        fp = tf.keras.backend.sum((1 - y_true_f) * y_pred_f)\n",
    "        fn = tf.keras.backend.sum(y_true_f * (1 - y_pred_f))\n",
    "        tversky_idx = (tp + smooth) / (tp + alpha * fp + beta * fn + smooth)\n",
    "        total_loss += weight * (1 - tversky_idx)\n",
    "    return total_loss / sum(CLASS_WEIGHTS)\n",
    "\n",
    "\n",
    "def combo_loss(y_true, y_pred):\n",
    "    \"\"\"Combined loss: Dice + Focal + Tversky.\"\"\"\n",
    "    dice = weighted_dice_loss(y_true, y_pred)\n",
    "    focal = focal_loss(y_true, y_pred)\n",
    "    tversky = tversky_loss(y_true, y_pred)\n",
    "    total = (LOSS_WEIGHTS['dice'] * dice + \n",
    "             LOSS_WEIGHTS['focal'] * focal + \n",
    "             LOSS_WEIGHTS['tversky'] * tversky)\n",
    "    return total / sum(LOSS_WEIGHTS.values())\n",
    "\n",
    "\n",
    "def dice_coefficient(y_true, y_pred):\n",
    "    \"\"\"Overall dice coefficient.\"\"\"\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.cast(y_pred, tf.float32)\n",
    "    y_true_f = tf.keras.backend.flatten(y_true)\n",
    "    y_pred_f = tf.keras.backend.flatten(y_pred)\n",
    "    intersection = tf.keras.backend.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + 1e-6) / (tf.keras.backend.sum(y_true_f) + tf.keras.backend.sum(y_pred_f) + 1e-6)\n",
    "\n",
    "\n",
    "def dice_liver(y_true, y_pred):\n",
    "    return dice_coefficient_per_class(y_true, y_pred, 1)\n",
    "\n",
    "\n",
    "def dice_tumor(y_true, y_pred):\n",
    "    return dice_coefficient_per_class(y_true, y_pred, 2)\n",
    "\n",
    "\n",
    "print(\"Loss functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Setup Test Files (same split as training)\n",
    "# =============================================================================\n",
    "\n",
    "DATA_DIR = 'preprocessed_patches_v2'\n",
    "NUM_CLASSES = 3\n",
    "SEED = 42\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "# Get test files (same split as training)\n",
    "all_files = sorted([os.path.join(DATA_DIR, f) for f in os.listdir(DATA_DIR) if f.endswith('.npz')])\n",
    "np.random.seed(SEED)\n",
    "indices = np.random.permutation(len(all_files))\n",
    "train_end = int(len(all_files) * 0.70)\n",
    "val_end = train_end + int(len(all_files) * 0.15)\n",
    "test_files = [all_files[i] for i in indices[val_end:]]\n",
    "\n",
    "print(f\"Total files: {len(all_files)}\")\n",
    "print(f\"Test set: {len(test_files)} files ({len(test_files) * 20} patches)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Load Combo Loss Model + Warmup\n",
    "# =============================================================================\n",
    "\n",
    "MODEL_PATH = 'checkpoints/best_model_combo_loss.keras'\n",
    "\n",
    "print(f\"Loading model from {MODEL_PATH}...\")\n",
    "model = tf.keras.models.load_model(\n",
    "    MODEL_PATH,\n",
    "    custom_objects={\n",
    "        'combo_loss': combo_loss,\n",
    "        'weighted_dice_loss': weighted_dice_loss,\n",
    "        'focal_loss': focal_loss,\n",
    "        'tversky_loss': tversky_loss,\n",
    "        'dice_coefficient': dice_coefficient,\n",
    "        'dice_liver': dice_liver,\n",
    "        'dice_tumor': dice_tumor\n",
    "    }\n",
    ")\n",
    "print(\"Model loaded successfully!\")\n",
    "print(f\"Model parameters: {model.count_params():,}\")\n",
    "\n",
    "# Warmup prediction to trigger XLA compilation\n",
    "print(\"\\nWarming up model (XLA compilation - this takes 1-2 min)...\")\n",
    "warmup_start = time.time()\n",
    "dummy_input = np.zeros((1, 128, 128, 128, 1), dtype=np.float32)\n",
    "_ = model.predict(dummy_input, verbose=0)\n",
    "print(f\"Warmup done in {time.time()-warmup_start:.1f}s - ready for evaluation!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Compute Confusion Matrix\n",
    "# =============================================================================\n",
    "\n",
    "print(\"Computing confusion matrix on test set...\")\n",
    "print(f\"Processing {len(test_files)} files ({len(test_files)*20} patches)...\\n\")\n",
    "\n",
    "cm = np.zeros((NUM_CLASSES, NUM_CLASSES), dtype=np.int64)\n",
    "total_patches = 0\n",
    "start_time = time.time()\n",
    "\n",
    "for file_idx, filepath in enumerate(test_files):\n",
    "    file_start = time.time()\n",
    "    data = np.load(filepath)\n",
    "    patches = data['patches'].astype(np.float32) / 255.0\n",
    "    segs = data['segmentations']\n",
    "    \n",
    "    # Process in batches\n",
    "    for i in range(0, len(patches), BATCH_SIZE):\n",
    "        x = patches[i:i+BATCH_SIZE][..., np.newaxis]\n",
    "        y_true = segs[i:i+BATCH_SIZE]\n",
    "        \n",
    "        pred = model.predict(x, verbose=0)\n",
    "        y_pred = np.argmax(pred, axis=-1)\n",
    "        \n",
    "        # Accumulate confusion matrix\n",
    "        cm += confusion_matrix(y_true.flatten(), y_pred.flatten(), labels=[0, 1, 2])\n",
    "        total_patches += len(x)\n",
    "    \n",
    "    file_time = time.time() - file_start\n",
    "    total_time = time.time() - start_time\n",
    "    eta = (total_time / (file_idx + 1)) * (len(test_files) - file_idx - 1)\n",
    "    print(f\"  [{file_idx+1:2d}/{len(test_files)}] {os.path.basename(filepath)}: {file_time:.1f}s (ETA: {eta:.0f}s)\")\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"DONE! Total time: {time.time()-start_time:.1f}s\")\n",
    "print(f\"Total voxels evaluated: {cm.sum():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Plot Confusion Matrix\n",
    "# =============================================================================\n",
    "\n",
    "class_names = ['Background', 'Liver', 'Tumor']\n",
    "cm_norm = cm.astype('float') / cm.sum(axis=1, keepdims=True) * 100\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Raw counts\n",
    "im1 = axes[0].imshow(cm, cmap='Blues')\n",
    "axes[0].set_title('Confusion Matrix (Counts)', fontsize=14)\n",
    "axes[0].set_xlabel('Predicted', fontsize=12)\n",
    "axes[0].set_ylabel('True', fontsize=12)\n",
    "axes[0].set_xticks(range(3))\n",
    "axes[0].set_yticks(range(3))\n",
    "axes[0].set_xticklabels(class_names)\n",
    "axes[0].set_yticklabels(class_names)\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        axes[0].text(j, i, f'{cm[i,j]:,}', ha='center', va='center', fontsize=10)\n",
    "plt.colorbar(im1, ax=axes[0])\n",
    "\n",
    "# Normalized (percentages)\n",
    "im2 = axes[1].imshow(cm_norm, cmap='Blues', vmin=0, vmax=100)\n",
    "axes[1].set_title('Confusion Matrix (% by True Class)', fontsize=14)\n",
    "axes[1].set_xlabel('Predicted', fontsize=12)\n",
    "axes[1].set_ylabel('True', fontsize=12)\n",
    "axes[1].set_xticks(range(3))\n",
    "axes[1].set_yticks(range(3))\n",
    "axes[1].set_xticklabels(class_names)\n",
    "axes[1].set_yticklabels(class_names)\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        axes[1].text(j, i, f'{cm_norm[i,j]:.1f}%', ha='center', va='center', fontsize=10)\n",
    "plt.colorbar(im2, ax=axes[1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('confusion_matrix_combo.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nConfusion matrix saved to confusion_matrix_combo.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Print Per-Class Metrics\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"PER-CLASS METRICS (Combo Loss Model)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Class':<12} {'Precision':>10} {'Recall':>10} {'F1':>10} {'Dice':>10}\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "metrics_dict = {}\n",
    "for i, name in enumerate(class_names):\n",
    "    tp = cm[i, i]\n",
    "    fp = cm[:, i].sum() - tp\n",
    "    fn = cm[i, :].sum() - tp\n",
    "    \n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    dice = (2 * tp) / (2 * tp + fp + fn) if (2 * tp + fp + fn) > 0 else 0\n",
    "    \n",
    "    metrics_dict[name] = {'precision': precision, 'recall': recall, 'f1': f1, 'dice': dice}\n",
    "    print(f\"{name:<12} {precision:>10.4f} {recall:>10.4f} {f1:>10.4f} {dice:>10.4f}\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Overall accuracy\n",
    "accuracy = np.trace(cm) / cm.sum()\n",
    "print(f\"\\nOverall Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Compare with Baseline (if available)\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMPARISON WITH BASELINE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Baseline metrics from original model (best_model_v2_augment.keras)\n",
    "baseline = {\n",
    "    'Tumor': {'recall': 0.527, 'dice': 0.65},  # From training history\n",
    "    'Liver': {'recall': 0.90, 'dice': 0.90}\n",
    "}\n",
    "\n",
    "print(f\"\\n{'Metric':<20} {'Baseline':>12} {'Combo Loss':>12} {'Change':>12}\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "# Tumor metrics\n",
    "tumor_recall_change = metrics_dict['Tumor']['recall'] - baseline['Tumor']['recall']\n",
    "tumor_dice_change = metrics_dict['Tumor']['dice'] - baseline['Tumor']['dice']\n",
    "print(f\"{'Tumor Recall':<20} {baseline['Tumor']['recall']:>12.4f} {metrics_dict['Tumor']['recall']:>12.4f} {tumor_recall_change:>+12.4f}\")\n",
    "print(f\"{'Tumor Dice':<20} {baseline['Tumor']['dice']:>12.4f} {metrics_dict['Tumor']['dice']:>12.4f} {tumor_dice_change:>+12.4f}\")\n",
    "\n",
    "# Liver metrics\n",
    "liver_recall_change = metrics_dict['Liver']['recall'] - baseline['Liver']['recall']\n",
    "liver_dice_change = metrics_dict['Liver']['dice'] - baseline['Liver']['dice']\n",
    "print(f\"{'Liver Recall':<20} {baseline['Liver']['recall']:>12.4f} {metrics_dict['Liver']['recall']:>12.4f} {liver_recall_change:>+12.4f}\")\n",
    "print(f\"{'Liver Dice':<20} {baseline['Liver']['dice']:>12.4f} {metrics_dict['Liver']['dice']:>12.4f} {liver_dice_change:>+12.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "if tumor_recall_change > 0:\n",
    "    print(f\"SUCCESS: Tumor recall improved by {tumor_recall_change*100:.1f}%!\")\n",
    "else:\n",
    "    print(f\"Tumor recall decreased by {abs(tumor_recall_change)*100:.1f}%\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Visualize Sample Predictions\n",
    "# =============================================================================\n",
    "\n",
    "def visualize_prediction(model, filepath, patch_idx=0, slice_idx=64):\n",
    "    \"\"\"Visualize a single prediction with ground truth.\"\"\"\n",
    "    data = np.load(filepath)\n",
    "    patch = data['patches'][patch_idx].astype(np.float32) / 255.0\n",
    "    seg_true = data['segmentations'][patch_idx]\n",
    "    \n",
    "    # Predict\n",
    "    pred = model.predict(patch[np.newaxis, ..., np.newaxis], verbose=0)\n",
    "    seg_pred = np.argmax(pred[0], axis=-1)\n",
    "    \n",
    "    # Plot\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    axes[0].imshow(patch[:, :, slice_idx], cmap='gray')\n",
    "    axes[0].set_title('CT Slice')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    axes[1].imshow(seg_true[:, :, slice_idx], cmap='jet', vmin=0, vmax=2)\n",
    "    axes[1].set_title('Ground Truth')\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    axes[2].imshow(seg_pred[:, :, slice_idx], cmap='jet', vmin=0, vmax=2)\n",
    "    axes[2].set_title('Prediction')\n",
    "    axes[2].axis('off')\n",
    "    \n",
    "    plt.suptitle(f'{os.path.basename(filepath)} - Patch {patch_idx}, Slice {slice_idx}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize a few samples\n",
    "print(\"Sample predictions from test set:\")\n",
    "for i in range(min(3, len(test_files))):\n",
    "    visualize_prediction(model, test_files[i], patch_idx=0, slice_idx=64)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
