{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "0",
   "source": [
    "# Swin-UNETR Transfer Learning for Liver/Tumor Segmentation\n",
    "\n",
    "Using pre-trained Swin-UNETR from MONAI with weights trained on 5,050 CT scans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "1",
   "outputs": [],
   "source": [
    "# Install MONAI if needed\n",
    "# !pip install monai[all]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "2",
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "from monai.networks.nets import SwinUNETR\n",
    "from monai.losses import DiceCELoss\n",
    "from monai.metrics import DiceMetric\n",
    "from monai.inferers import sliding_window_inference\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "3",
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Configuration\n",
    "# =============================================================================\n",
    "\n",
    "CONFIG = {\n",
    "    # Data\n",
    "    'data_dir': 'preprocessed_patches_v2',\n",
    "    'num_classes': 3,\n",
    "    'input_size': (128, 128, 128),\n",
    "    \n",
    "    # Model\n",
    "    'feature_size': 48,  # 48 for base, 24 for small (less VRAM)\n",
    "    'use_pretrained': True,\n",
    "    \n",
    "    # Training\n",
    "    'batch_size': 1,  # Keep small for 12GB VRAM\n",
    "    'accumulation_steps': 4,  # Effective batch size = 4\n",
    "    'epochs': 100,\n",
    "    'lr': 1e-4,\n",
    "    'weight_decay': 1e-5,\n",
    "    \n",
    "    # Class weights for imbalanced data [bg, liver, tumor]\n",
    "    'class_weights': [0.1, 1.0, 5.0],\n",
    "    \n",
    "    # Misc\n",
    "    'seed': 42,\n",
    "    'num_workers': 2,\n",
    "    'checkpoint_dir': 'checkpoints_swin',\n",
    "}\n",
    "\n",
    "# Create checkpoint directory\n",
    "os.makedirs(CONFIG['checkpoint_dir'], exist_ok=True)\n",
    "\n",
    "# Set seed for reproducibility\n",
    "torch.manual_seed(CONFIG['seed'])\n",
    "np.random.seed(CONFIG['seed'])\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "4",
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Dataset\n",
    "# =============================================================================\n",
    "\n",
    "class LiverDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for liver/tumor segmentation using preprocessed .npz files.\n",
    "    Each file contains 20 patches of shape (128, 128, 128).\n",
    "    \"\"\"\n",
    "    def __init__(self, file_list, augment=False):\n",
    "        self.file_list = file_list\n",
    "        self.augment = augment\n",
    "        \n",
    "        # Build flat index: (file_idx, patch_idx)\n",
    "        self.indices = []\n",
    "        for file_idx, filepath in enumerate(file_list):\n",
    "            # Each file has 20 patches\n",
    "            for patch_idx in range(20):\n",
    "                self.indices.append((file_idx, patch_idx))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        file_idx, patch_idx = self.indices[idx]\n",
    "        data = np.load(self.file_list[file_idx])\n",
    "        \n",
    "        # Load patch and segmentation\n",
    "        image = data['patches'][patch_idx].astype(np.float32) / 255.0\n",
    "        label = data['segmentations'][patch_idx].astype(np.int64)\n",
    "        \n",
    "        # Add channel dimension: (128,128,128) -> (1,128,128,128)\n",
    "        image = image[np.newaxis, ...]\n",
    "        \n",
    "        # Simple augmentation\n",
    "        if self.augment:\n",
    "            # Random flip along each axis (50% chance each)\n",
    "            if np.random.random() > 0.5:\n",
    "                image = np.flip(image, axis=1).copy()\n",
    "                label = np.flip(label, axis=0).copy()\n",
    "            if np.random.random() > 0.5:\n",
    "                image = np.flip(image, axis=2).copy()\n",
    "                label = np.flip(label, axis=1).copy()\n",
    "            if np.random.random() > 0.5:\n",
    "                image = np.flip(image, axis=3).copy()\n",
    "                label = np.flip(label, axis=2).copy()\n",
    "            \n",
    "            # Random intensity shift\n",
    "            if np.random.random() > 0.5:\n",
    "                shift = np.random.uniform(-0.1, 0.1)\n",
    "                image = np.clip(image + shift, 0, 1)\n",
    "        \n",
    "        return torch.from_numpy(image), torch.from_numpy(label)\n",
    "\n",
    "\n",
    "# Get file list and split\n",
    "all_files = sorted([os.path.join(CONFIG['data_dir'], f) \n",
    "                    for f in os.listdir(CONFIG['data_dir']) if f.endswith('.npz')])\n",
    "\n",
    "np.random.seed(CONFIG['seed'])\n",
    "indices = np.random.permutation(len(all_files))\n",
    "train_end = int(len(all_files) * 0.70)\n",
    "val_end = train_end + int(len(all_files) * 0.15)\n",
    "\n",
    "train_files = [all_files[i] for i in indices[:train_end]]\n",
    "val_files = [all_files[i] for i in indices[train_end:val_end]]\n",
    "test_files = [all_files[i] for i in indices[val_end:]]\n",
    "\n",
    "print(f\"Total files: {len(all_files)}\")\n",
    "print(f\"Train: {len(train_files)} files ({len(train_files)*20} patches)\")\n",
    "print(f\"Val: {len(val_files)} files ({len(val_files)*20} patches)\")\n",
    "print(f\"Test: {len(test_files)} files ({len(test_files)*20} patches)\")\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_dataset = LiverDataset(train_files, augment=True)\n",
    "val_dataset = LiverDataset(val_files, augment=False)\n",
    "test_dataset = LiverDataset(test_files, augment=False)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=CONFIG['batch_size'], \n",
    "                          shuffle=True, num_workers=CONFIG['num_workers'], pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=CONFIG['batch_size'], \n",
    "                        shuffle=False, num_workers=CONFIG['num_workers'], pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=CONFIG['batch_size'], \n",
    "                         shuffle=False, num_workers=CONFIG['num_workers'], pin_memory=True)\n",
    "\n",
    "print(f\"\\nTrain batches: {len(train_loader)}\")\n",
    "print(f\"Val batches: {len(val_loader)}\")\n",
    "print(f\"Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "5",
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Model: Swin-UNETR with Pre-trained Weights\n",
    "# =============================================================================\n",
    "\n",
    "model = SwinUNETR(\n",
    "    img_size=CONFIG['input_size'],\n",
    "    in_channels=1,\n",
    "    out_channels=CONFIG['num_classes'],\n",
    "    feature_size=CONFIG['feature_size'],\n",
    "    use_checkpoint=True,  # Gradient checkpointing to save VRAM\n",
    "    spatial_dims=3,\n",
    ")\n",
    "\n",
    "if CONFIG['use_pretrained']:\n",
    "    print(\"Loading pre-trained weights...\")\n",
    "    # Download pre-trained weights (trained on 5050 CT volumes)\n",
    "    weight_path = os.path.join(CONFIG['checkpoint_dir'], 'swin_unetr_pretrained.pt')\n",
    "    \n",
    "    if not os.path.exists(weight_path):\n",
    "        print(\"Downloading pre-trained weights (~400MB)...\")\n",
    "        import urllib.request\n",
    "        url = \"https://github.com/Project-MONAI/MONAI-extra-test-data/releases/download/0.8.1/swin_unetr.base_5000ep_f48_lr2e-4_pretrained.pt\"\n",
    "        urllib.request.urlretrieve(url, weight_path)\n",
    "        print(\"Download complete!\")\n",
    "    \n",
    "    # Load weights (only encoder, not the segmentation head)\n",
    "    pretrained_dict = torch.load(weight_path, map_location='cpu')\n",
    "    model_dict = model.state_dict()\n",
    "    \n",
    "    # Filter out segmentation head weights (we have different num_classes)\n",
    "    pretrained_dict = {k: v for k, v in pretrained_dict.items() \n",
    "                       if k in model_dict and v.shape == model_dict[k].shape}\n",
    "    \n",
    "    model_dict.update(pretrained_dict)\n",
    "    model.load_state_dict(model_dict)\n",
    "    print(f\"Loaded {len(pretrained_dict)}/{len(model_dict)} pre-trained layers\")\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "6",
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Loss Function and Optimizer\n",
    "# =============================================================================\n",
    "\n",
    "# Combined Dice + Cross-Entropy loss with class weights\n",
    "class_weights = torch.tensor(CONFIG['class_weights'], dtype=torch.float32, device=device)\n",
    "\n",
    "loss_fn = DiceCELoss(\n",
    "    to_onehot_y=True,\n",
    "    softmax=True,\n",
    "    ce_weight=class_weights,\n",
    ")\n",
    "\n",
    "# Optimizer with weight decay\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=CONFIG['lr'],\n",
    "    weight_decay=CONFIG['weight_decay']\n",
    ")\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer, \n",
    "    T_max=CONFIG['epochs'],\n",
    "    eta_min=1e-6\n",
    ")\n",
    "\n",
    "# Mixed precision scaler\n",
    "scaler = GradScaler()\n",
    "\n",
    "# Dice metric for evaluation\n",
    "dice_metric = DiceMetric(include_background=False, reduction='mean_batch')\n",
    "\n",
    "print(\"Loss: DiceCE with class weights\", CONFIG['class_weights'])\n",
    "print(f\"Optimizer: AdamW (lr={CONFIG['lr']}, wd={CONFIG['weight_decay']})\")\n",
    "print(f\"Scheduler: CosineAnnealing\")\n",
    "print(f\"Mixed precision: Enabled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "7",
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Training Functions\n",
    "# =============================================================================\n",
    "\n",
    "def train_epoch(model, loader, optimizer, loss_fn, scaler, accumulation_steps=1):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    pbar = tqdm(loader, desc='Training')\n",
    "    for batch_idx, (images, labels) in enumerate(pbar):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Mixed precision forward pass\n",
    "        with autocast():\n",
    "            outputs = model(images)\n",
    "            loss = loss_fn(outputs, labels.unsqueeze(1)) / accumulation_steps\n",
    "        \n",
    "        # Backward pass with gradient scaling\n",
    "        scaler.scale(loss).backward()\n",
    "        \n",
    "        # Gradient accumulation\n",
    "        if (batch_idx + 1) % accumulation_steps == 0:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        total_loss += loss.item() * accumulation_steps\n",
    "        pbar.set_postfix({'loss': f'{loss.item() * accumulation_steps:.4f}'})\n",
    "    \n",
    "    return total_loss / len(loader)\n",
    "\n",
    "\n",
    "def validate(model, loader, loss_fn, dice_metric):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    dice_metric.reset()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(loader, desc='Validation'):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            with autocast():\n",
    "                outputs = model(images)\n",
    "                loss = loss_fn(outputs, labels.unsqueeze(1))\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Compute dice\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            # One-hot encode for dice metric\n",
    "            preds_onehot = torch.nn.functional.one_hot(preds, CONFIG['num_classes']).permute(0, 4, 1, 2, 3)\n",
    "            labels_onehot = torch.nn.functional.one_hot(labels, CONFIG['num_classes']).permute(0, 4, 1, 2, 3)\n",
    "            dice_metric(preds_onehot, labels_onehot)\n",
    "    \n",
    "    dice_scores = dice_metric.aggregate()\n",
    "    dice_metric.reset()\n",
    "    \n",
    "    return total_loss / len(loader), dice_scores\n",
    "\n",
    "\n",
    "print(\"Training functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "8",
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Training Loop\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Starting Swin-UNETR Training\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Epochs: {CONFIG['epochs']}\")\n",
    "print(f\"Batch size: {CONFIG['batch_size']} x {CONFIG['accumulation_steps']} = {CONFIG['batch_size'] * CONFIG['accumulation_steps']} effective\")\n",
    "print(f\"Learning rate: {CONFIG['lr']}\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'val_loss': [],\n",
    "    'val_dice_liver': [],\n",
    "    'val_dice_tumor': [],\n",
    "    'lr': []\n",
    "}\n",
    "\n",
    "best_dice_tumor = 0\n",
    "patience = 15\n",
    "patience_counter = 0\n",
    "\n",
    "try:\n",
    "    for epoch in range(CONFIG['epochs']):\n",
    "        epoch_start = time.time()\n",
    "        \n",
    "        # Train\n",
    "        train_loss = train_epoch(\n",
    "            model, train_loader, optimizer, loss_fn, scaler, \n",
    "            accumulation_steps=CONFIG['accumulation_steps']\n",
    "        )\n",
    "        \n",
    "        # Validate\n",
    "        val_loss, dice_scores = validate(model, val_loader, loss_fn, dice_metric)\n",
    "        dice_liver = dice_scores[0].item()  # Class 1 (liver)\n",
    "        dice_tumor = dice_scores[1].item()  # Class 2 (tumor)\n",
    "        \n",
    "        # Update scheduler\n",
    "        scheduler.step()\n",
    "        current_lr = scheduler.get_last_lr()[0]\n",
    "        \n",
    "        # Save history\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_dice_liver'].append(dice_liver)\n",
    "        history['val_dice_tumor'].append(dice_tumor)\n",
    "        history['lr'].append(current_lr)\n",
    "        \n",
    "        epoch_time = time.time() - epoch_start\n",
    "        \n",
    "        # Print progress\n",
    "        print(f\"Epoch {epoch+1:3d}/{CONFIG['epochs']} | \"\n",
    "              f\"Train Loss: {train_loss:.4f} | \"\n",
    "              f\"Val Loss: {val_loss:.4f} | \"\n",
    "              f\"Dice Liver: {dice_liver:.4f} | \"\n",
    "              f\"Dice Tumor: {dice_tumor:.4f} | \"\n",
    "              f\"LR: {current_lr:.2e} | \"\n",
    "              f\"Time: {epoch_time:.0f}s\")\n",
    "        \n",
    "        # Save best model\n",
    "        if dice_tumor > best_dice_tumor:\n",
    "            best_dice_tumor = dice_tumor\n",
    "            patience_counter = 0\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'best_dice_tumor': best_dice_tumor,\n",
    "                'config': CONFIG,\n",
    "            }, os.path.join(CONFIG['checkpoint_dir'], 'best_model.pt'))\n",
    "            print(f\"  >> New best tumor Dice: {best_dice_tumor:.4f} - Model saved!\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        # Save latest checkpoint\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'history': history,\n",
    "        }, os.path.join(CONFIG['checkpoint_dir'], 'latest_checkpoint.pt'))\n",
    "        \n",
    "        # Early stopping\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"\\nEarly stopping at epoch {epoch+1} (no improvement for {patience} epochs)\")\n",
    "            break\n",
    "        \n",
    "        print()\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Training interrupted by user\")\n",
    "    print(\"=\"*60)\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'history': history,\n",
    "    }, os.path.join(CONFIG['checkpoint_dir'], 'interrupted_model.pt'))\n",
    "    print(f\"Model saved to {CONFIG['checkpoint_dir']}/interrupted_model.pt\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"Training Complete! Best Tumor Dice: {best_dice_tumor:.4f}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Save history\n",
    "with open(os.path.join(CONFIG['checkpoint_dir'], 'training_history.json'), 'w') as f:\n",
    "    json.dump(history, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "9",
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Plot Training History\n",
    "# =============================================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Loss\n",
    "axes[0].plot(history['train_loss'], label='Train')\n",
    "axes[0].plot(history['val_loss'], label='Val')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training & Validation Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Dice Scores\n",
    "axes[1].plot(history['val_dice_liver'], label='Liver')\n",
    "axes[1].plot(history['val_dice_tumor'], label='Tumor')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Dice Score')\n",
    "axes[1].set_title('Validation Dice Scores')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "# Learning Rate\n",
    "axes[2].plot(history['lr'])\n",
    "axes[2].set_xlabel('Epoch')\n",
    "axes[2].set_ylabel('Learning Rate')\n",
    "axes[2].set_title('Learning Rate Schedule')\n",
    "axes[2].set_yscale('log')\n",
    "axes[2].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(CONFIG['checkpoint_dir'], 'training_history.png'), dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "10",
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Load Best Model and Evaluate on Test Set\n",
    "# =============================================================================\n",
    "\n",
    "print(\"Loading best model for evaluation...\")\n",
    "checkpoint = torch.load(os.path.join(CONFIG['checkpoint_dir'], 'best_model.pt'))\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "print(f\"Loaded model from epoch {checkpoint['epoch']+1} with tumor Dice: {checkpoint['best_dice_tumor']:.4f}\")\n",
    "\n",
    "# Evaluate on test set\n",
    "model.eval()\n",
    "cm = np.zeros((CONFIG['num_classes'], CONFIG['num_classes']), dtype=np.int64)\n",
    "\n",
    "print(\"\\nEvaluating on test set...\")\n",
    "with torch.no_grad():\n",
    "    for images, labels in tqdm(test_loader, desc='Testing'):\n",
    "        images = images.to(device)\n",
    "        \n",
    "        with autocast():\n",
    "            outputs = model(images)\n",
    "        \n",
    "        preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "        labels = labels.numpy()\n",
    "        \n",
    "        cm += confusion_matrix(labels.flatten(), preds.flatten(), labels=[0, 1, 2])\n",
    "\n",
    "print(f\"\\nTotal voxels evaluated: {cm.sum():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "11",
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Plot Confusion Matrix and Metrics\n",
    "# =============================================================================\n",
    "\n",
    "class_names = ['Background', 'Liver', 'Tumor']\n",
    "cm_norm = cm.astype('float') / cm.sum(axis=1, keepdims=True) * 100\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Raw counts\n",
    "im1 = axes[0].imshow(cm, cmap='Blues')\n",
    "axes[0].set_title('Confusion Matrix (Counts)', fontsize=14)\n",
    "axes[0].set_xlabel('Predicted', fontsize=12)\n",
    "axes[0].set_ylabel('True', fontsize=12)\n",
    "axes[0].set_xticks(range(3))\n",
    "axes[0].set_yticks(range(3))\n",
    "axes[0].set_xticklabels(class_names)\n",
    "axes[0].set_yticklabels(class_names)\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        axes[0].text(j, i, f'{cm[i,j]:,}', ha='center', va='center', fontsize=10)\n",
    "plt.colorbar(im1, ax=axes[0])\n",
    "\n",
    "# Normalized\n",
    "im2 = axes[1].imshow(cm_norm, cmap='Blues', vmin=0, vmax=100)\n",
    "axes[1].set_title('Confusion Matrix (% by True Class)', fontsize=14)\n",
    "axes[1].set_xlabel('Predicted', fontsize=12)\n",
    "axes[1].set_ylabel('True', fontsize=12)\n",
    "axes[1].set_xticks(range(3))\n",
    "axes[1].set_yticks(range(3))\n",
    "axes[1].set_xticklabels(class_names)\n",
    "axes[1].set_yticklabels(class_names)\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        axes[1].text(j, i, f'{cm_norm[i,j]:.1f}%', ha='center', va='center', fontsize=10)\n",
    "plt.colorbar(im2, ax=axes[1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(CONFIG['checkpoint_dir'], 'confusion_matrix.png'), dpi=150)\n",
    "plt.show()\n",
    "\n",
    "# Print metrics\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TEST SET RESULTS - Swin-UNETR\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Class':<12} {'Precision':>10} {'Recall':>10} {'F1':>10} {'Dice':>10}\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "for i, name in enumerate(class_names):\n",
    "    tp = cm[i, i]\n",
    "    fp = cm[:, i].sum() - tp\n",
    "    fn = cm[i, :].sum() - tp\n",
    "    \n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    dice = (2 * tp) / (2 * tp + fp + fn) if (2 * tp + fp + fn) > 0 else 0\n",
    "    \n",
    "    print(f\"{name:<12} {precision:>10.4f} {recall:>10.4f} {f1:>10.4f} {dice:>10.4f}\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "accuracy = np.trace(cm) / cm.sum()\n",
    "print(f\"\\nOverall Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "12",
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Compare with U-Net Baseline\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMPARISON: Swin-UNETR vs 3D U-Net Baseline\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Your U-Net results (from previous evaluation)\n",
    "unet_results = {\n",
    "    'Background': {'dice': 0.9734},\n",
    "    'Liver': {'dice': 0.9036},\n",
    "    'Tumor': {'dice': 0.6539},\n",
    "}\n",
    "\n",
    "# Calculate Swin-UNETR results\n",
    "swin_results = {}\n",
    "for i, name in enumerate(class_names):\n",
    "    tp = cm[i, i]\n",
    "    fp = cm[:, i].sum() - tp\n",
    "    fn = cm[i, :].sum() - tp\n",
    "    dice = (2 * tp) / (2 * tp + fp + fn) if (2 * tp + fp + fn) > 0 else 0\n",
    "    swin_results[name] = {'dice': dice}\n",
    "\n",
    "print(f\"{'Class':<12} {'U-Net':>12} {'Swin-UNETR':>12} {'Improvement':>12}\")\n",
    "print(\"-\"*50)\n",
    "for name in class_names:\n",
    "    unet_dice = unet_results[name]['dice']\n",
    "    swin_dice = swin_results[name]['dice']\n",
    "    improvement = (swin_dice - unet_dice) * 100\n",
    "    arrow = '↑' if improvement > 0 else '↓' if improvement < 0 else '→'\n",
    "    print(f\"{name:<12} {unet_dice:>11.4f} {swin_dice:>12.4f} {arrow:>6}{abs(improvement):>5.1f}%\")\n",
    "print(\"=\"*50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
