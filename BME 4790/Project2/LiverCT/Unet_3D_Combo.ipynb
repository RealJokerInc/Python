{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ls13_Ajd97nM"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uFN1rRtr97nR"
   },
   "outputs": [],
   "source": "import os\nimport sys\nimport json\nfrom pathlib import Path\nfrom typing import List\n\n# Set XLA flag to use fallback convolution algorithms (prevents OOM during autotuning)\nos.environ['XLA_FLAGS'] = '--xla_gpu_strict_conv_algorithm_picker=false'\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport cv2\nfrom tensorflow.keras import layers, models, regularizers\nfrom tensorflow.keras.utils import to_categorical\n\nimport nibabel as nib\nimport SimpleITK as sitk\nfrom scipy.ndimage import rotate as scipy_rotate\n\nfrom PIL import Image, ImageOps, ImageFilter\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n\n\ntf.keras.mixed_precision.set_global_policy('mixed_bfloat16')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "NS8Mmgwc46Sb",
    "outputId": "c5f7ea46-a7b2-4f81-bc4c-927f78fc5b3d"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 3D U-Net for Liver CT Segmentation\n",
    "# =============================================================================\n",
    "# Input: (128, 128, 128, 1) - single channel CT patch\n",
    "# Output: (128, 128, 128, num_classes) - segmentation mask\n",
    "# Classes: 0=background, 1=liver, 2=tumor\n",
    "\n",
    "def double_conv_block_3d(x, n_filters, kernel_size=3):\n",
    "    \"\"\"Two consecutive 3D convolutions with BatchNorm and ReLU.\"\"\"\n",
    "    x = layers.Conv3D(n_filters, kernel_size, padding=\"same\", kernel_initializer=\"he_normal\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "    x = layers.Conv3D(n_filters, kernel_size, padding=\"same\", kernel_initializer=\"he_normal\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def downsample_block_3d(x, n_filters, dropout_rate=0.3):\n",
    "    \"\"\"Encoder block: double conv + max pool + dropout.\"\"\"\n",
    "    f = double_conv_block_3d(x, n_filters)\n",
    "    p = layers.MaxPool3D(pool_size=(2, 2, 2))(f)\n",
    "    p = layers.Dropout(dropout_rate)(p)\n",
    "    return f, p\n",
    "\n",
    "\n",
    "def upsample_block_3d(x, skip_features, n_filters, dropout_rate=0.5):\n",
    "    \"\"\"Decoder block: upsample + concat skip + double conv.\"\"\"\n",
    "    x = layers.Conv3DTranspose(n_filters, kernel_size=2, strides=2, padding=\"same\")(x)\n",
    "    x = layers.concatenate([x, skip_features])\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "    x = double_conv_block_3d(x, n_filters)\n",
    "    return x\n",
    "\n",
    "\n",
    "def build_3d_unet(input_shape=(128, 128, 128, 1), num_classes=3, base_filters=32):\n",
    "    \"\"\"\n",
    "    Build a 3D U-Net model for volumetric segmentation.\n",
    "    \n",
    "    Parameters:\n",
    "    - input_shape: (D, H, W, C) - default (128, 128, 128, 1)\n",
    "    - num_classes: Number of output classes (3: background, liver, tumor)\n",
    "    - base_filters: Starting number of filters (doubles each level)\n",
    "    \n",
    "    Architecture (base_filters=32):\n",
    "        Encoder: 32 -> 64 -> 128 -> 256\n",
    "        Bottleneck: 512\n",
    "        Decoder: 256 -> 128 -> 64 -> 32\n",
    "        \n",
    "    Returns:\n",
    "    - model: Keras Model\n",
    "    \"\"\"\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    \n",
    "    # Encoder path\n",
    "    f1, p1 = downsample_block_3d(inputs, base_filters)        \n",
    "    f2, p2 = downsample_block_3d(p1, base_filters * 2)        \n",
    "    f3, p3 = downsample_block_3d(p2, base_filters * 4)        \n",
    "    f4, p4 = downsample_block_3d(p3, base_filters * 8)\n",
    "    \n",
    "    # Bottleneck\n",
    "    bottleneck = double_conv_block_3d(p4, base_filters * 16)  \n",
    "    \n",
    "    # Decoder path\n",
    "    u1 = upsample_block_3d(bottleneck, f4, base_filters * 8)  \n",
    "    u2 = upsample_block_3d(u1, f3, base_filters * 4)         \n",
    "    u3 = upsample_block_3d(u2, f2, base_filters * 2)         \n",
    "    u4 = upsample_block_3d(u3, f1, base_filters)            \n",
    "    \n",
    "\n",
    "    outputs = layers.Conv3D(num_classes, kernel_size=1, padding=\"same\", activation=\"softmax\")(u4)\n",
    "    \n",
    "    model = models.Model(inputs, outputs, name=\"3D-UNet\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 131 volume files in preprocessed_patches_v2/\n",
      "\n",
      "Split (by patient):\n",
      "  Train: 91 files (1820 patches)\n",
      "  Val:   19 files (380 patches)\n",
      "  Test:  21 files (420 patches)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Get File List and Split into Train/Val/Test\n",
    "# =============================================================================\n",
    "\n",
    "DATA_DIR = 'preprocessed_patches_v2'  # Updated to use tumor-centered patches\n",
    "NUM_CLASSES = 3\n",
    "SEED = 42\n",
    "\n",
    "TRAIN_RATIO = 0.70\n",
    "VAL_RATIO = 0.15\n",
    "TEST_RATIO = 0.15\n",
    "\n",
    "# Get all .npz files\n",
    "all_files = sorted([os.path.join(DATA_DIR, f) for f in os.listdir(DATA_DIR) if f.endswith('.npz')])\n",
    "print(f\"Found {len(all_files)} volume files in {DATA_DIR}/\")\n",
    "\n",
    "# Shuffle and split\n",
    "np.random.seed(SEED)\n",
    "indices = np.random.permutation(len(all_files))\n",
    "\n",
    "train_end = int(len(all_files) * TRAIN_RATIO)\n",
    "val_end = train_end + int(len(all_files) * VAL_RATIO)\n",
    "\n",
    "train_files = [all_files[i] for i in indices[:train_end]]\n",
    "val_files = [all_files[i] for i in indices[train_end:val_end]]\n",
    "test_files = [all_files[i] for i in indices[val_end:]]\n",
    "\n",
    "print(f\"\\nSplit (by patient):\")\n",
    "print(f\"  Train: {len(train_files)} files ({len(train_files)*20} patches)\")\n",
    "print(f\"  Val:   {len(val_files)} files ({len(val_files)*20} patches)\")\n",
    "print(f\"  Test:  {len(test_files)} files ({len(test_files)*20} patches)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# Online Data Augmentation Functions\n# =============================================================================\n\ndef augment_rotation_3d(volume, segmentation, max_angle=15):\n    \"\"\"\n    Apply random 3D rotation to volume and segmentation.\n    Rotates around a random axis by a random angle within ±max_angle degrees.\n    \"\"\"\n    angle_x = np.random.uniform(-max_angle, max_angle)\n    angle_y = np.random.uniform(-max_angle, max_angle)\n    angle_z = np.random.uniform(-max_angle, max_angle)\n    \n    vol_rotated = scipy_rotate(volume, angle_z, axes=(0, 1), reshape=False, order=1, mode='constant', cval=0)\n    vol_rotated = scipy_rotate(vol_rotated, angle_y, axes=(0, 2), reshape=False, order=1, mode='constant', cval=0)\n    vol_rotated = scipy_rotate(vol_rotated, angle_x, axes=(1, 2), reshape=False, order=1, mode='constant', cval=0)\n    \n    seg_rotated = scipy_rotate(segmentation, angle_z, axes=(0, 1), reshape=False, order=0, mode='constant', cval=0)\n    seg_rotated = scipy_rotate(seg_rotated, angle_y, axes=(0, 2), reshape=False, order=0, mode='constant', cval=0)\n    seg_rotated = scipy_rotate(seg_rotated, angle_x, axes=(1, 2), reshape=False, order=0, mode='constant', cval=0)\n    \n    return vol_rotated, seg_rotated\n\n\ndef augment_gamma(volume, gamma_range=(0.7, 1.5)):\n    gamma = np.random.uniform(gamma_range[0], gamma_range[1])\n    return np.power(np.clip(volume, 0, 1), gamma)\n\n\ndef augment_gaussian_noise(volume, sigma_range=(0, 0.05)):\n    sigma = np.random.uniform(sigma_range[0], sigma_range[1])\n    noise = np.random.normal(0, sigma, volume.shape)\n    return np.clip(volume + noise, 0, 1)\n\n\ndef augment_brightness(volume, delta_range=(-0.1, 0.1)):\n    delta = np.random.uniform(delta_range[0], delta_range[1])\n    return np.clip(volume + delta, 0, 1)\n\n\ndef apply_augmentation(volume, segmentation, augment=True):\n    if not augment:\n        return volume, segmentation\n    \n    if np.random.random() < 0.5:\n        volume, segmentation = augment_rotation_3d(volume, segmentation, max_angle=15)\n    \n    if np.random.random() < 0.5:\n        volume = augment_gamma(volume, gamma_range=(0.7, 1.5))\n    \n    if np.random.random() < 0.5:\n        volume = augment_gaussian_noise(volume, sigma_range=(0, 0.05))\n    \n    if np.random.random() < 0.3:\n        volume = augment_brightness(volume, delta_range=(-0.1, 0.1))\n    \n    return volume, segmentation\n\n\n# =============================================================================\n# File-Based Data Generator (Original Working Version)\n# =============================================================================\n\nclass VolumeGenerator(tf.keras.utils.Sequence):\n    \"\"\"\n    Loads patches from .npz files with configurable batch size.\n    Each file contains: patches (20, 128, 128, 128), segmentations (20, 128, 128, 128)\n    \"\"\"\n    \n    def __init__(self, files, batch_size=2, num_classes=3, shuffle=True, augment=False):\n        self.files = files\n        self.batch_size = batch_size\n        self.num_classes = num_classes\n        self.shuffle = shuffle\n        self.augment = augment\n        \n        self.batch_indices = []\n        for file_idx, filepath in enumerate(files):\n            data = np.load(filepath)\n            n_patches = len(data['patches'])\n            for start in range(0, n_patches, batch_size):\n                self.batch_indices.append((file_idx, start))\n        \n        self.on_epoch_end()\n    \n    def __len__(self):\n        return len(self.batch_indices)\n    \n    def __getitem__(self, idx):\n        file_idx, patch_start = self.batch_indices[self.indices[idx]]\n        data = np.load(self.files[file_idx])\n        \n        patch_end = patch_start + self.batch_size\n        \n        x = data['patches'][patch_start:patch_end].astype(np.float32) / 255.0\n        y = data['segmentations'][patch_start:patch_end]\n        \n        if self.augment:\n            x_aug = []\n            y_aug = []\n            for i in range(len(x)):\n                vol_aug, seg_aug = apply_augmentation(x[i], y[i], augment=True)\n                x_aug.append(vol_aug)\n                y_aug.append(seg_aug)\n            x = np.array(x_aug, dtype=np.float32)\n            y = np.array(y_aug)\n        \n        x = x[..., np.newaxis]\n        y = to_categorical(y, num_classes=self.num_classes)\n        \n        return x, y\n    \n    def on_epoch_end(self):\n        self.indices = np.arange(len(self.batch_indices))\n        if self.shuffle:\n            np.random.shuffle(self.indices)\n\n\n# Create generators\nBATCH_SIZE = 2\n\ntrain_gen = VolumeGenerator(train_files, batch_size=BATCH_SIZE, num_classes=NUM_CLASSES, shuffle=True, augment=True)\nval_gen = VolumeGenerator(val_files, batch_size=BATCH_SIZE, num_classes=NUM_CLASSES, shuffle=False, augment=False)\ntest_gen = VolumeGenerator(test_files, batch_size=BATCH_SIZE, num_classes=NUM_CLASSES, shuffle=False, augment=False)\n\nprint(f\"Batch size: {BATCH_SIZE}\")\nprint(f\"Train: {len(train_gen)} batches (with augmentation: rotation ±15°, gamma, noise)\")\nprint(f\"Val:   {len(val_gen)} batches (no augmentation)\")\nprint(f\"Test:  {len(test_gen)} batches (no augmentation)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# Combined Loss Function: Dice + Focal + Tversky\n# =============================================================================\n\n# Class weights: [background, liver, tumor]\nCLASS_WEIGHTS = [0.1, 1.0, 30.0]  # Increased tumor weight\n\n# Loss combination weights\nLOSS_WEIGHTS = {\n    'dice': 1.0,\n    'focal': 1.0,\n    'tversky': 1.0,\n}\n\n# Tversky parameters (alpha > beta favors recall over precision)\nTVERSKY_ALPHA = 0.3  # Weight for false positives\nTVERSKY_BETA = 0.7   # Weight for false negatives (higher = better recall)\n\n# Focal loss parameters\nFOCAL_GAMMA = 2.0    # Focus on hard examples (higher = more focus)\n\n\ndef dice_coefficient_per_class(y_true, y_pred, class_idx, smooth=1e-6):\n    \"\"\"Dice coefficient for a single class.\"\"\"\n    y_true = tf.cast(y_true, tf.float32)\n    y_pred = tf.cast(y_pred, tf.float32)\n    y_true_c = y_true[..., class_idx]\n    y_pred_c = y_pred[..., class_idx]\n    y_true_f = tf.keras.backend.flatten(y_true_c)\n    y_pred_f = tf.keras.backend.flatten(y_pred_c)\n    intersection = tf.keras.backend.sum(y_true_f * y_pred_f)\n    return (2. * intersection + smooth) / (tf.keras.backend.sum(y_true_f) + tf.keras.backend.sum(y_pred_f) + smooth)\n\n\ndef weighted_dice_loss(y_true, y_pred):\n    \"\"\"Weighted dice loss - higher weight for tumor class.\"\"\"\n    y_true = tf.cast(y_true, tf.float32)\n    y_pred = tf.cast(y_pred, tf.float32)\n    \n    total_loss = 0.0\n    for class_idx, weight in enumerate(CLASS_WEIGHTS):\n        dice = dice_coefficient_per_class(y_true, y_pred, class_idx)\n        total_loss += weight * (1 - dice)\n    \n    return total_loss / sum(CLASS_WEIGHTS)\n\n\ndef focal_loss(y_true, y_pred, gamma=FOCAL_GAMMA):\n    \"\"\"\n    Focal loss for handling class imbalance.\n    Focuses training on hard-to-classify examples.\n    \n    FL(p) = -alpha * (1-p)^gamma * log(p)\n    \"\"\"\n    y_true = tf.cast(y_true, tf.float32)\n    y_pred = tf.cast(y_pred, tf.float32)\n    \n    # Clip predictions to prevent log(0)\n    y_pred = tf.clip_by_value(y_pred, 1e-7, 1 - 1e-7)\n    \n    # Calculate focal weight: (1 - p)^gamma\n    focal_weight = tf.pow(1 - y_pred, gamma)\n    \n    # Cross entropy\n    ce = -y_true * tf.math.log(y_pred)\n    \n    # Apply class weights\n    class_weights_tensor = tf.constant(CLASS_WEIGHTS, dtype=tf.float32)\n    class_weights_tensor = class_weights_tensor / tf.reduce_sum(class_weights_tensor)\n    \n    # Weighted focal loss\n    focal = focal_weight * ce * class_weights_tensor\n    \n    return tf.reduce_mean(tf.reduce_sum(focal, axis=-1))\n\n\ndef tversky_loss(y_true, y_pred, alpha=TVERSKY_ALPHA, beta=TVERSKY_BETA, smooth=1e-6):\n    \"\"\"\n    Tversky loss for controlling precision/recall tradeoff.\n    \n    Tversky Index = TP / (TP + alpha*FP + beta*FN)\n    \n    - alpha = beta = 0.5 -> equivalent to Dice\n    - alpha < beta -> penalizes FN more (improves recall)\n    - alpha > beta -> penalizes FP more (improves precision)\n    \n    For tumor segmentation, we want higher recall, so beta > alpha.\n    \"\"\"\n    y_true = tf.cast(y_true, tf.float32)\n    y_pred = tf.cast(y_pred, tf.float32)\n    \n    total_loss = 0.0\n    for class_idx, weight in enumerate(CLASS_WEIGHTS):\n        y_true_c = y_true[..., class_idx]\n        y_pred_c = y_pred[..., class_idx]\n        \n        y_true_f = tf.keras.backend.flatten(y_true_c)\n        y_pred_f = tf.keras.backend.flatten(y_pred_c)\n        \n        # True positives, false positives, false negatives\n        tp = tf.keras.backend.sum(y_true_f * y_pred_f)\n        fp = tf.keras.backend.sum((1 - y_true_f) * y_pred_f)\n        fn = tf.keras.backend.sum(y_true_f * (1 - y_pred_f))\n        \n        # Tversky index\n        tversky_idx = (tp + smooth) / (tp + alpha * fp + beta * fn + smooth)\n        total_loss += weight * (1 - tversky_idx)\n    \n    return total_loss / sum(CLASS_WEIGHTS)\n\n\ndef combo_loss(y_true, y_pred):\n    \"\"\"\n    Combined loss: Dice + Focal + Tversky\n    \n    This combination provides:\n    - Dice: Overall segmentation quality\n    - Focal: Focus on hard examples (small tumors)\n    - Tversky: Better recall for minority class (tumors)\n    \"\"\"\n    dice = weighted_dice_loss(y_true, y_pred)\n    focal = focal_loss(y_true, y_pred)\n    tversky = tversky_loss(y_true, y_pred)\n    \n    total = (LOSS_WEIGHTS['dice'] * dice + \n             LOSS_WEIGHTS['focal'] * focal + \n             LOSS_WEIGHTS['tversky'] * tversky)\n    \n    return total / sum(LOSS_WEIGHTS.values())\n\n\n# =============================================================================\n# Metrics\n# =============================================================================\n\ndef dice_coefficient(y_true, y_pred):\n    \"\"\"Overall dice coefficient.\"\"\"\n    y_true = tf.cast(y_true, tf.float32)\n    y_pred = tf.cast(y_pred, tf.float32)\n    y_true_f = tf.keras.backend.flatten(y_true)\n    y_pred_f = tf.keras.backend.flatten(y_pred)\n    intersection = tf.keras.backend.sum(y_true_f * y_pred_f)\n    return (2. * intersection + 1e-6) / (tf.keras.backend.sum(y_true_f) + tf.keras.backend.sum(y_pred_f) + 1e-6)\n\n\ndef dice_liver(y_true, y_pred):\n    \"\"\"Dice coefficient for liver (class 1).\"\"\"\n    return dice_coefficient_per_class(y_true, y_pred, 1)\n\n\ndef dice_tumor(y_true, y_pred):\n    \"\"\"Dice coefficient for tumor (class 2).\"\"\"\n    return dice_coefficient_per_class(y_true, y_pred, 2)\n\n\nprint(\"Loss functions defined:\")\nprint(f\"  - Dice Loss (weight: {LOSS_WEIGHTS['dice']})\")\nprint(f\"  - Focal Loss (weight: {LOSS_WEIGHTS['focal']}, gamma: {FOCAL_GAMMA})\")\nprint(f\"  - Tversky Loss (weight: {LOSS_WEIGHTS['tversky']}, alpha: {TVERSKY_ALPHA}, beta: {TVERSKY_BETA})\")\nprint(f\"\\nClass weights: Background={CLASS_WEIGHTS[0]}, Liver={CLASS_WEIGHTS[1]}, Tumor={CLASS_WEIGHTS[2]}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# Build and Compile Model (Fine-tuning from pre-trained weights)\n# =============================================================================\n\nmodel = build_3d_unet(input_shape=(128, 128, 128, 1), num_classes=NUM_CLASSES, base_filters=24)\n\n# Load pre-trained weights from ORIGINAL best model (not the overwritten one!)\nPRETRAINED_PATH = 'checkpoints/best_model_v2_augment.keras'  # Original 65% dice tumor model\nprint(f\"Loading pre-trained weights from {PRETRAINED_PATH}...\")\nmodel.load_weights(PRETRAINED_PATH)\nprint(\"Weights loaded successfully!\")\n\n# Compile with combo loss and lower learning rate for fine-tuning\nmodel.compile(\n    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),  # Lower LR for fine-tuning\n    loss=combo_loss,  # Combined: Dice + Focal + Tversky\n    metrics=[dice_coefficient, dice_liver, dice_tumor]\n)\n\nprint(f\"\\nFine-tuning with Combo Loss (Dice + Focal + Tversky)\")\nprint(f\"Learning rate: 1e-5 (reduced for fine-tuning)\")\nmodel.summary()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# Train Model with Crash Protection\n# =============================================================================\n\nEPOCHS = 30\nCHECKPOINT_DIR = 'checkpoints'\nos.makedirs(CHECKPOINT_DIR, exist_ok=True)\n\ncallbacks = [\n    tf.keras.callbacks.ModelCheckpoint(\n        filepath=os.path.join(CHECKPOINT_DIR, 'best_model_combo_loss.keras'),\n        monitor='val_dice_tumor',\n        mode='max',\n        save_best_only=True,\n        verbose=1\n    ),\n    tf.keras.callbacks.ModelCheckpoint(\n        filepath=os.path.join(CHECKPOINT_DIR, 'latest_checkpoint.keras'),\n        save_best_only=False,\n        verbose=0\n    ),\n    tf.keras.callbacks.ReduceLROnPlateau(\n        monitor='val_loss',\n        factor=0.5,\n        patience=5,\n        min_lr=1e-7,\n        verbose=1\n    ),\n    tf.keras.callbacks.EarlyStopping(\n        monitor='val_dice_tumor',\n        mode='max',\n        patience=10,\n        restore_best_weights=True,\n        verbose=1\n    )\n]\n\nhistory = None\ntraining_completed = False\n\ntry:\n    print(\"Starting training with combo loss...\")\n    print(f\"Best model will be saved to: {CHECKPOINT_DIR}/best_model_combo_loss.keras\")\n    print(\"-\" * 60)\n    \n    history = model.fit(\n        train_gen,\n        validation_data=val_gen,\n        epochs=EPOCHS,\n        callbacks=callbacks,\n        verbose=1\n    )\n    training_completed = True\n    print(\"\\nTraining completed successfully!\")\n    \nexcept KeyboardInterrupt:\n    print(\"\\n\" + \"=\" * 60)\n    print(\"TRAINING INTERRUPTED BY USER\")\n    print(\"=\" * 60)\n    print(\"Saving emergency checkpoint...\")\n    model.save(os.path.join(CHECKPOINT_DIR, 'interrupted_model.keras'))\n    print(f\"Model saved to {CHECKPOINT_DIR}/interrupted_model.keras\")\n    \nexcept Exception as e:\n    print(\"\\n\" + \"=\" * 60)\n    print(f\"TRAINING CRASHED: {type(e).__name__}\")\n    print(\"=\" * 60)\n    print(f\"Error: {e}\")\n    print(\"\\nAttempting emergency save...\")\n    try:\n        model.save(os.path.join(CHECKPOINT_DIR, 'crash_recovery_model.keras'))\n        print(f\"Model saved to {CHECKPOINT_DIR}/crash_recovery_model.keras\")\n    except Exception as save_error:\n        print(f\"Emergency save failed: {save_error}\")\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"CHECKPOINT SUMMARY\")\nprint(\"=\" * 60)\nif os.path.exists(CHECKPOINT_DIR):\n    for f in sorted(os.listdir(CHECKPOINT_DIR)):\n        fpath = os.path.join(CHECKPOINT_DIR, f)\n        size_mb = os.path.getsize(fpath) / 1024**2\n        print(f\"  {f}: {size_mb:.1f} MB\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Save Final Model\n",
    "# =============================================================================\n",
    "\n",
    "# Check if training completed and save final model\n",
    "if training_completed:\n",
    "    try:\n",
    "        model.save('final_model.keras')\n",
    "        print(\"Final model saved to final_model.keras\")\n",
    "        \n",
    "        # Also save training history\n",
    "        if history is not None:\n",
    "            history_dict = {key: [float(v) for v in values] for key, values in history.history.items()}\n",
    "            with open('training_history.json', 'w') as f:\n",
    "                json.dump(history_dict, f, indent=2)\n",
    "            print(\"Training history saved to training_history.json\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving final model: {e}\")\n",
    "        print(\"Best model should be available in checkpoints/best_model.keras\")\n",
    "else:\n",
    "    print(\"Training did not complete normally.\")\n",
    "    print(\"Check checkpoints/ directory for saved models:\")\n",
    "    print(\"  - best_model.keras (best validation tumor dice)\")\n",
    "    print(\"  - latest_checkpoint.keras (last completed epoch)\")\n",
    "    print(\"  - interrupted_model.keras or crash_recovery_model.keras (if crashed)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing confusion matrix on test set (memory-efficient)...\n",
      "Processing 105 batches...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 100\u001b[39m\n\u001b[32m     98\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mComputing confusion matrix on test set (memory-efficient)...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     99\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mProcessing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(test_gen)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m batches...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m100\u001b[39m cm = compute_confusion_matrix_incremental(\u001b[43mmodel\u001b[49m, test_gen, num_classes=NUM_CLASSES)\n\u001b[32m    101\u001b[39m plot_confusion_matrix(cm)\n",
      "\u001b[31mNameError\u001b[39m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Confusion Matrix (Memory-Efficient Incremental Computation)\n",
    "# =============================================================================\n",
    "\n",
    "def compute_confusion_matrix_incremental(model, generator, num_classes=3, num_batches=None):\n",
    "    \"\"\"\n",
    "    Compute confusion matrix incrementally to avoid memory issues.\n",
    "    Instead of storing all predictions (~11GB), accumulates a 3x3 matrix (~72 bytes).\n",
    "    \"\"\"\n",
    "    cm = np.zeros((num_classes, num_classes), dtype=np.int64)\n",
    "    \n",
    "    n_batches = num_batches if num_batches else len(generator)\n",
    "    \n",
    "    for i in range(n_batches):\n",
    "        x, y = generator[i]\n",
    "        pred = model.predict(x, verbose=0)\n",
    "        \n",
    "        # Get class labels\n",
    "        y_true = np.argmax(y, axis=-1).flatten()\n",
    "        y_pred = np.argmax(pred, axis=-1).flatten()\n",
    "        \n",
    "        # Accumulate confusion matrix directly (no storage of all predictions)\n",
    "        cm += confusion_matrix(y_true, y_pred, labels=list(range(num_classes)))\n",
    "        \n",
    "        if (i + 1) % 50 == 0:\n",
    "            print(f\"  Processed {i+1}/{n_batches} batches...\")\n",
    "    \n",
    "    return cm\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(cm, class_names=['Background', 'Liver', 'Tumor']):\n",
    "    \"\"\"Plot confusion matrix with percentages.\"\"\"\n",
    "    # Normalize by row (true labels)\n",
    "    cm_norm = cm.astype('float') / cm.sum(axis=1, keepdims=True) * 100\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Raw counts\n",
    "    im1 = axes[0].imshow(cm, cmap='Blues')\n",
    "    axes[0].set_title('Confusion Matrix (Counts)')\n",
    "    axes[0].set_xlabel('Predicted')\n",
    "    axes[0].set_ylabel('True')\n",
    "    axes[0].set_xticks(range(len(class_names)))\n",
    "    axes[0].set_yticks(range(len(class_names)))\n",
    "    axes[0].set_xticklabels(class_names)\n",
    "    axes[0].set_yticklabels(class_names)\n",
    "    \n",
    "    for i in range(len(class_names)):\n",
    "        for j in range(len(class_names)):\n",
    "            axes[0].text(j, i, f'{cm[i, j]:,}', ha='center', va='center', fontsize=10)\n",
    "    \n",
    "    plt.colorbar(im1, ax=axes[0])\n",
    "    \n",
    "    # Normalized (percentages)\n",
    "    im2 = axes[1].imshow(cm_norm, cmap='Blues', vmin=0, vmax=100)\n",
    "    axes[1].set_title('Confusion Matrix (% by True Class)')\n",
    "    axes[1].set_xlabel('Predicted')\n",
    "    axes[1].set_ylabel('True')\n",
    "    axes[1].set_xticks(range(len(class_names)))\n",
    "    axes[1].set_yticks(range(len(class_names)))\n",
    "    axes[1].set_xticklabels(class_names)\n",
    "    axes[1].set_yticklabels(class_names)\n",
    "    \n",
    "    for i in range(len(class_names)):\n",
    "        for j in range(len(class_names)):\n",
    "            axes[1].text(j, i, f'{cm_norm[i, j]:.1f}%', ha='center', va='center', fontsize=10)\n",
    "    \n",
    "    plt.colorbar(im2, ax=axes[1])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('confusion_matrix.png', dpi=150)\n",
    "    plt.show()\n",
    "    \n",
    "    # Print per-class metrics\n",
    "    print(\"\\nPer-Class Metrics:\")\n",
    "    print(\"-\" * 50)\n",
    "    for i, name in enumerate(class_names):\n",
    "        tp = cm[i, i]\n",
    "        fn = cm[i, :].sum() - tp\n",
    "        fp = cm[:, i].sum() - tp\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        print(f\"{name:12s}: Precision={precision:.4f}, Recall={recall:.4f}, F1={f1:.4f}\")\n",
    "    \n",
    "    # Print dice-like metrics\n",
    "    print(\"\\nDice Coefficients (from confusion matrix):\")\n",
    "    print(\"-\" * 50)\n",
    "    for i, name in enumerate(class_names):\n",
    "        tp = cm[i, i]\n",
    "        fp = cm[:, i].sum() - tp\n",
    "        fn = cm[i, :].sum() - tp\n",
    "        dice = (2 * tp) / (2 * tp + fp + fn) if (2 * tp + fp + fn) > 0 else 0\n",
    "        print(f\"{name:12s}: Dice={dice:.4f}\")\n",
    "\n",
    "\n",
    "# Compute and plot\n",
    "print(\"Computing confusion matrix on test set (memory-efficient)...\")\n",
    "print(f\"Processing {len(test_gen)} batches...\")\n",
    "cm = compute_confusion_matrix_incremental(model, test_gen, num_classes=NUM_CLASSES)\n",
    "plot_confusion_matrix(cm)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}