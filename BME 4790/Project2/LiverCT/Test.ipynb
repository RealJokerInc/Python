{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Model Evaluation - Confusion Matrix\n",
    "Evaluates the trained 3D U-Net model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-15 18:11:55.625855: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-12-15 18:11:55.658660: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-12-15 18:11:56.283174: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss functions defined.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Define Loss Functions (needed for model loading)\n",
    "# =============================================================================\n",
    "\n",
    "CLASS_WEIGHTS = [0.1, 1, 20.0]\n",
    "\n",
    "def dice_coefficient_per_class(y_true, y_pred, class_idx):\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.cast(y_pred, tf.float32)\n",
    "    y_true_c = y_true[..., class_idx]\n",
    "    y_pred_c = y_pred[..., class_idx]\n",
    "    y_true_f = tf.keras.backend.flatten(y_true_c)\n",
    "    y_pred_f = tf.keras.backend.flatten(y_pred_c)\n",
    "    intersection = tf.keras.backend.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection) / (tf.keras.backend.sum(y_true_f) + tf.keras.backend.sum(y_pred_f))\n",
    "\n",
    "def weighted_dice_loss(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.cast(y_pred, tf.float32)\n",
    "    total_loss = 0.0\n",
    "    for class_idx, weight in enumerate(CLASS_WEIGHTS):\n",
    "        dice = dice_coefficient_per_class(y_true, y_pred, class_idx)\n",
    "        total_loss += weight * (1 - dice)\n",
    "    return total_loss / sum(CLASS_WEIGHTS)\n",
    "\n",
    "def dice_coefficient(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.cast(y_pred, tf.float32)\n",
    "    y_true_f = tf.keras.backend.flatten(y_true)\n",
    "    y_pred_f = tf.keras.backend.flatten(y_pred)\n",
    "    intersection = tf.keras.backend.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection) / (tf.keras.backend.sum(y_true_f) + tf.keras.backend.sum(y_pred_f))\n",
    "\n",
    "def dice_liver(y_true, y_pred):\n",
    "    return dice_coefficient_per_class(y_true, y_pred, 1)\n",
    "\n",
    "def dice_tumor(y_true, y_pred):\n",
    "    return dice_coefficient_per_class(y_true, y_pred, 2)\n",
    "\n",
    "print(\"Loss functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total files: 131\n",
      "Test set: 21 files (420 patches)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Setup Test Files\n",
    "# =============================================================================\n",
    "\n",
    "DATA_DIR = 'preprocessed_patches_v2'\n",
    "NUM_CLASSES = 3\n",
    "SEED = 42\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "# Get test files (same split as training)\n",
    "all_files = sorted([os.path.join(DATA_DIR, f) for f in os.listdir(DATA_DIR) if f.endswith('.npz')])\n",
    "np.random.seed(SEED)\n",
    "indices = np.random.permutation(len(all_files))\n",
    "train_end = int(len(all_files) * 0.70)\n",
    "val_end = train_end + int(len(all_files) * 0.15)\n",
    "test_files = [all_files[i] for i in indices[val_end:]]\n",
    "\n",
    "print(f\"Total files: {len(all_files)}\")\n",
    "print(f\"Test set: {len(test_files)} files ({len(test_files) * 20} patches)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from checkpoints/best_model_v2_augment.keras...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1765840317.097091  280811 gpu_device.cc:2020] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9064 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4080 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n",
      "Model parameters: 12,708,315\n",
      "\n",
      "Warming up model (XLA compilation - this takes 1-2 min)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-15 18:11:58.334416: E tensorflow/core/util/util.cc:131] oneDNN supports DT_BFLOAT16 only on platforms with AVX-512. Falling back to the default Eigen-based implementation if present.\n",
      "2025-12-15 18:11:58.340392: I external/local_xla/xla/service/service.cc:163] XLA service 0x7c80a402dee0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2025-12-15 18:11:58.340406: I external/local_xla/xla/service/service.cc:171]   StreamExecutor device (0): NVIDIA GeForce RTX 4080 Laptop GPU, Compute Capability 8.9\n",
      "2025-12-15 18:11:58.352536: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2025-12-15 18:11:58.471221: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:473] Loaded cuDNN version 91700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warmup done in 5.2s - ready for evaluation!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1765840323.241902  280944 device_compiler.h:196] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Load Best Model + Warmup\n",
    "# =============================================================================\n",
    "\n",
    "MODEL_PATH = 'checkpoints/best_model_v2_augment.keras'\n",
    "\n",
    "print(f\"Loading model from {MODEL_PATH}...\")\n",
    "model = tf.keras.models.load_model(\n",
    "    MODEL_PATH,\n",
    "    custom_objects={\n",
    "        'weighted_dice_loss': weighted_dice_loss,\n",
    "        'dice_coefficient': dice_coefficient,\n",
    "        'dice_liver': dice_liver,\n",
    "        'dice_tumor': dice_tumor\n",
    "    }\n",
    ")\n",
    "print(\"Model loaded successfully!\")\n",
    "print(f\"Model parameters: {model.count_params():,}\")\n",
    "\n",
    "# Warmup prediction to trigger XLA compilation (this takes 1-2 minutes)\n",
    "print(\"\\nWarming up model (XLA compilation - this takes 1-2 min)...\")\n",
    "warmup_start = time.time()\n",
    "dummy_input = np.zeros((1, 128, 128, 128, 1), dtype=np.float32)\n",
    "_ = model.predict(dummy_input, verbose=0)\n",
    "print(f\"Warmup done in {time.time()-warmup_start:.1f}s - ready for evaluation!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing confusion matrix on test set...\n",
      "Processing 21 files (420 patches)...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-15 18:12:07.304204: E external/local_xla/xla/service/slow_operation_alarm.cc:73] Trying algorithm eng0{} for conv (bf16[4,24,128,128,128]{4,3,2,1,0}, u8[0]{0}) custom-call(bf16[4,24,128,128,128]{4,3,2,1,0}, bf16[24,24,3,3,3]{4,3,2,1,0}, bf16[24]{0}), window={size=3x3x3 pad=1_1x1_1x1_1}, dim_labels=bf012_oi012->bf012, custom_call_target=\"__cudnn$convBiasActivationForward\", backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"activation_mode\":\"kNone\",\"conv_result_scale\":1,\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false,\"reification_cost\":[]} is taking a while...\n",
      "2025-12-15 18:12:07.425005: E external/local_xla/xla/service/slow_operation_alarm.cc:140] The operation took 1.121013414s\n",
      "Trying algorithm eng0{} for conv (bf16[4,24,128,128,128]{4,3,2,1,0}, u8[0]{0}) custom-call(bf16[4,24,128,128,128]{4,3,2,1,0}, bf16[24,24,3,3,3]{4,3,2,1,0}, bf16[24]{0}), window={size=3x3x3 pad=1_1x1_1x1_1}, dim_labels=bf012_oi012->bf012, custom_call_target=\"__cudnn$convBiasActivationForward\", backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"activation_mode\":\"kNone\",\"conv_result_scale\":1,\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false,\"reification_cost\":[]} is taking a while...\n",
      "2025-12-15 18:12:12.230877: E external/local_xla/xla/service/slow_operation_alarm.cc:73] Trying algorithm eng0{} for conv (bf16[4,48,64,64,64]{4,3,2,1,0}, u8[0]{0}) custom-call(bf16[4,96,64,64,64]{4,3,2,1,0}, bf16[48,96,3,3,3]{4,3,2,1,0}, bf16[48]{0}), window={size=3x3x3 pad=1_1x1_1x1_1}, dim_labels=bf012_oi012->bf012, custom_call_target=\"__cudnn$convBiasActivationForward\", backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"activation_mode\":\"kNone\",\"conv_result_scale\":1,\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false,\"reification_cost\":[]} is taking a while...\n",
      "2025-12-15 18:12:12.281861: E external/local_xla/xla/service/slow_operation_alarm.cc:140] The operation took 1.051141447s\n",
      "Trying algorithm eng0{} for conv (bf16[4,48,64,64,64]{4,3,2,1,0}, u8[0]{0}) custom-call(bf16[4,96,64,64,64]{4,3,2,1,0}, bf16[48,96,3,3,3]{4,3,2,1,0}, bf16[48]{0}), window={size=3x3x3 pad=1_1x1_1x1_1}, dim_labels=bf012_oi012->bf012, custom_call_target=\"__cudnn$convBiasActivationForward\", backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"activation_mode\":\"kNone\",\"conv_result_scale\":1,\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false,\"reification_cost\":[]} is taking a while...\n",
      "2025-12-15 18:12:16.714778: E external/local_xla/xla/service/slow_operation_alarm.cc:73] Trying algorithm eng0{} for conv (bf16[4,24,128,128,128]{4,3,2,1,0}, u8[0]{0}) custom-call(bf16[4,48,128,128,128]{4,3,2,1,0}, bf16[24,48,3,3,3]{4,3,2,1,0}, bf16[24]{0}), window={size=3x3x3 pad=1_1x1_1x1_1}, dim_labels=bf012_oi012->bf012, custom_call_target=\"__cudnn$convBiasActivationForward\", backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"activation_mode\":\"kNone\",\"conv_result_scale\":1,\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false,\"reification_cost\":[]} is taking a while...\n",
      "2025-12-15 18:12:17.939577: E external/local_xla/xla/service/slow_operation_alarm.cc:140] The operation took 2.224957763s\n",
      "Trying algorithm eng0{} for conv (bf16[4,24,128,128,128]{4,3,2,1,0}, u8[0]{0}) custom-call(bf16[4,48,128,128,128]{4,3,2,1,0}, bf16[24,48,3,3,3]{4,3,2,1,0}, bf16[24]{0}), window={size=3x3x3 pad=1_1x1_1x1_1}, dim_labels=bf012_oi012->bf012, custom_call_target=\"__cudnn$convBiasActivationForward\", backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"activation_mode\":\"kNone\",\"conv_result_scale\":1,\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false,\"reification_cost\":[]} is taking a while...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [ 1/21] volume_052.npz: 18.2s (ETA: 363s)\n",
      "  [ 2/21] volume_021.npz: 2.8s (ETA: 199s)\n",
      "  [ 3/21] volume_002.npz: 2.7s (ETA: 142s)\n",
      "  [ 4/21] volume_023.npz: 2.7s (ETA: 112s)\n",
      "  [ 5/21] volume_103.npz: 2.7s (ETA: 93s)\n",
      "  [ 6/21] volume_099.npz: 2.7s (ETA: 79s)\n",
      "  [ 7/21] volume_116.npz: 2.7s (ETA: 69s)\n",
      "  [ 8/21] volume_087.npz: 2.7s (ETA: 60s)\n",
      "  [ 9/21] volume_119.npz: 2.7s (ETA: 53s)\n",
      "  [10/21] volume_074.npz: 2.7s (ETA: 47s)\n",
      "  [11/21] volume_086.npz: 2.6s (ETA: 41s)\n",
      "  [12/21] volume_082.npz: 2.6s (ETA: 36s)\n",
      "  [13/21] volume_121.npz: 2.7s (ETA: 31s)\n",
      "  [14/21] volume_130.npz: 2.7s (ETA: 26s)\n",
      "  [15/21] volume_020.npz: 2.7s (ETA: 22s)\n",
      "  [16/21] volume_060.npz: 2.7s (ETA: 18s)\n",
      "  [17/21] volume_071.npz: 2.6s (ETA: 14s)\n",
      "  [18/21] volume_106.npz: 2.6s (ETA: 11s)\n",
      "  [19/21] volume_014.npz: 2.6s (ETA: 7s)\n",
      "  [20/21] volume_092.npz: 2.7s (ETA: 3s)\n",
      "  [21/21] volume_102.npz: 2.6s (ETA: 0s)\n",
      "\n",
      "==================================================\n",
      "DONE! Total time: 71.4s\n",
      "Total voxels evaluated: 880,803,840\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Compute Confusion Matrix\n",
    "# =============================================================================\n",
    "\n",
    "print(\"Computing confusion matrix on test set...\")\n",
    "print(f\"Processing {len(test_files)} files ({len(test_files)*20} patches)...\\n\")\n",
    "\n",
    "cm = np.zeros((NUM_CLASSES, NUM_CLASSES), dtype=np.int64)\n",
    "total_patches = 0\n",
    "start_time = time.time()\n",
    "\n",
    "for file_idx, filepath in enumerate(test_files):\n",
    "    file_start = time.time()\n",
    "    data = np.load(filepath)\n",
    "    patches = data['patches'].astype(np.float32) / 255.0\n",
    "    segs = data['segmentations']\n",
    "    \n",
    "    # Process in batches\n",
    "    for i in range(0, len(patches), BATCH_SIZE):\n",
    "        x = patches[i:i+BATCH_SIZE][..., np.newaxis]\n",
    "        y_true = segs[i:i+BATCH_SIZE]\n",
    "        \n",
    "        pred = model.predict(x, verbose=0)\n",
    "        y_pred = np.argmax(pred, axis=-1)\n",
    "        \n",
    "        # Accumulate confusion matrix\n",
    "        cm += confusion_matrix(y_true.flatten(), y_pred.flatten(), labels=[0, 1, 2])\n",
    "        total_patches += len(x)\n",
    "    \n",
    "    file_time = time.time() - file_start\n",
    "    total_time = time.time() - start_time\n",
    "    eta = (total_time / (file_idx + 1)) * (len(test_files) - file_idx - 1)\n",
    "    print(f\"  [{file_idx+1:2d}/{len(test_files)}] {os.path.basename(filepath)}: {file_time:.1f}s (ETA: {eta:.0f}s)\")\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"DONE! Total time: {time.time()-start_time:.1f}s\")\n",
    "print(f\"Total voxels evaluated: {cm.sum():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# Plot Confusion Matrix\n# =============================================================================\n\nclass_names = ['Background', 'Liver', 'Tumor']\ncm_norm = cm.astype('float') / cm.sum(axis=1, keepdims=True) * 100\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Raw counts\nim1 = axes[0].imshow(cm, cmap='Blues')\naxes[0].set_title('Confusion Matrix (Counts)', fontsize=14)\naxes[0].set_xlabel('Predicted', fontsize=12)\naxes[0].set_ylabel('True', fontsize=12)\naxes[0].set_xticks(range(3))\naxes[0].set_yticks(range(3))\naxes[0].set_xticklabels(class_names)\naxes[0].set_yticklabels(class_names)\nfor i in range(3):\n    for j in range(3):\n        axes[0].text(j, i, f'{cm[i,j]:,}', ha='center', va='center', fontsize=10)\ncbar1 = plt.colorbar(im1, ax=axes[0])\ncbar1.set_label('Voxel Count', fontsize=11)\n\n# Normalized (percentages)\nim2 = axes[1].imshow(cm_norm, cmap='Blues', vmin=0, vmax=100)\naxes[1].set_title('Confusion Matrix (% by True Class)', fontsize=14)\naxes[1].set_xlabel('Predicted', fontsize=12)\naxes[1].set_ylabel('True', fontsize=12)\naxes[1].set_xticks(range(3))\naxes[1].set_yticks(range(3))\naxes[1].set_xticklabels(class_names)\naxes[1].set_yticklabels(class_names)\nfor i in range(3):\n    for j in range(3):\n        axes[1].text(j, i, f'{cm_norm[i,j]:.1f}%', ha='center', va='center', fontsize=10)\ncbar2 = plt.colorbar(im2, ax=axes[1])\ncbar2.set_label('Percentage (%)', fontsize=11)\n\nplt.tight_layout()\nplt.savefig('confusion_matrix.png', dpi=150, bbox_inches='tight')\nplt.show()\n\nprint(\"\\nConfusion matrix saved to confusion_matrix.png\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "PER-CLASS METRICS\n",
      "============================================================\n",
      "Class         Precision     Recall         F1       Dice\n",
      "------------------------------------------------------------\n",
      "Background       0.9807     0.9662     0.9734     0.9734\n",
      "Liver            0.8702     0.9397     0.9036     0.9036\n",
      "Tumor            0.8618     0.5269     0.6539     0.6539\n",
      "============================================================\n",
      "\n",
      "Overall Accuracy: 0.9501 (95.01%)\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Print Per-Class Metrics\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"PER-CLASS METRICS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Class':<12} {'Precision':>10} {'Recall':>10} {'F1':>10} {'Dice':>10}\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "for i, name in enumerate(class_names):\n",
    "    tp = cm[i, i]\n",
    "    fp = cm[:, i].sum() - tp\n",
    "    fn = cm[i, :].sum() - tp\n",
    "    \n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    dice = (2 * tp) / (2 * tp + fp + fn) if (2 * tp + fp + fn) > 0 else 0\n",
    "    \n",
    "    print(f\"{name:<12} {precision:>10.4f} {recall:>10.4f} {f1:>10.4f} {dice:>10.4f}\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Overall accuracy\n",
    "accuracy = np.trace(cm) / cm.sum()\n",
    "print(f\"\\nOverall Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating visualization for test patients...\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Visualization: Original Patch, Ground Truth, Predicted Mask\n",
    "# =============================================================================\n",
    "\n",
    "def visualize_predictions(model, test_files, num_patients=5, slice_idx=64):\n",
    "    \"\"\"\n",
    "    Visualize predictions for multiple patients.\n",
    "    \n",
    "    Parameters:\n",
    "    - model: Trained model\n",
    "    - test_files: List of test file paths\n",
    "    - num_patients: Number of patients (rows) to display\n",
    "    - slice_idx: Which slice of the 3D volume to display (default: middle slice)\n",
    "    \"\"\"\n",
    "    num_patients = min(num_patients, len(test_files))\n",
    "    \n",
    "    fig, axes = plt.subplots(num_patients, 3, figsize=(12, 4 * num_patients))\n",
    "    \n",
    "    # Handle single patient case\n",
    "    if num_patients == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    # Column titles\n",
    "    col_titles = ['Original Patch', 'Ground Truth', 'Predicted Mask']\n",
    "    \n",
    "    # Color map for segmentation: 0=black (bg), 1=green (liver), 2=red (tumor)\n",
    "    seg_cmap = plt.cm.colors.ListedColormap(['black', 'green', 'red'])\n",
    "    \n",
    "    for row, filepath in enumerate(test_files[:num_patients]):\n",
    "        # Load data\n",
    "        data = np.load(filepath)\n",
    "        patches = data['patches'].astype(np.float32) / 255.0\n",
    "        segs = data['segmentations']\n",
    "        \n",
    "        # Use first patch from this patient (or one with tumor if available)\n",
    "        patch_idx = 0\n",
    "        for i in range(len(segs)):\n",
    "            if np.any(segs[i] == 2):  # Find patch with tumor\n",
    "                patch_idx = i\n",
    "                break\n",
    "        \n",
    "        # Get patch and segmentation\n",
    "        patch = patches[patch_idx]\n",
    "        gt = segs[patch_idx]\n",
    "        \n",
    "        # Predict\n",
    "        x = patch[np.newaxis, ..., np.newaxis]\n",
    "        pred = model.predict(x, verbose=0)\n",
    "        pred_mask = np.argmax(pred[0], axis=-1)\n",
    "        \n",
    "        # Get the slice (use middle or specified slice)\n",
    "        s = min(slice_idx, patch.shape[0] - 1)\n",
    "        \n",
    "        # Find slice with most tumor content if available\n",
    "        tumor_counts = [np.sum(gt[i] == 2) for i in range(gt.shape[0])]\n",
    "        if max(tumor_counts) > 0:\n",
    "            s = np.argmax(tumor_counts)\n",
    "        \n",
    "        # Plot original patch\n",
    "        axes[row, 0].imshow(patch[s], cmap='gray', vmin=0, vmax=1)\n",
    "        axes[row, 0].set_ylabel(f'Patient {row + 1}\\n{os.path.basename(filepath)[:15]}...', fontsize=10)\n",
    "        axes[row, 0].set_xticks([])\n",
    "        axes[row, 0].set_yticks([])\n",
    "        \n",
    "        # Plot ground truth\n",
    "        axes[row, 1].imshow(patch[s], cmap='gray', vmin=0, vmax=1, alpha=0.5)\n",
    "        axes[row, 1].imshow(gt[s], cmap=seg_cmap, vmin=0, vmax=2, alpha=0.5)\n",
    "        axes[row, 1].set_xticks([])\n",
    "        axes[row, 1].set_yticks([])\n",
    "        \n",
    "        # Plot prediction\n",
    "        axes[row, 2].imshow(patch[s], cmap='gray', vmin=0, vmax=1, alpha=0.5)\n",
    "        axes[row, 2].imshow(pred_mask[s], cmap=seg_cmap, vmin=0, vmax=2, alpha=0.5)\n",
    "        axes[row, 2].set_xticks([])\n",
    "        axes[row, 2].set_yticks([])\n",
    "        \n",
    "        # Add column titles on first row\n",
    "        if row == 0:\n",
    "            for col, title in enumerate(col_titles):\n",
    "                axes[row, col].set_title(title, fontsize=12, fontweight='bold')\n",
    "    \n",
    "    # Add legend\n",
    "    from matplotlib.patches import Patch\n",
    "    legend_elements = [\n",
    "        Patch(facecolor='black', label='Background'),\n",
    "        Patch(facecolor='green', label='Liver'),\n",
    "        Patch(facecolor='red', label='Tumor')\n",
    "    ]\n",
    "    fig.legend(handles=legend_elements, loc='lower center', ncol=3, fontsize=10, \n",
    "               bbox_to_anchor=(0.5, -0.02))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(bottom=0.08)\n",
    "    plt.savefig('prediction_visualization.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Visualization saved to prediction_visualization.png\")\n",
    "\n",
    "\n",
    "# Run visualization\n",
    "print(\"Generating visualization for test patients...\")\n",
    "visualize_predictions(model, test_files, num_patients=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Visualization: Single Patient - Multiple Slices (Axial View)\n",
    "# =============================================================================\n",
    "\n",
    "def visualize_single_patient_slices(model, test_files, patient_idx=0, num_slices=8):\n",
    "    \"\"\"\n",
    "    Visualize multiple slices from a single patient.\n",
    "    \n",
    "    Parameters:\n",
    "    - model: Trained model\n",
    "    - test_files: List of test file paths\n",
    "    - patient_idx: Which patient to visualize\n",
    "    - num_slices: Number of slices (rows) to display\n",
    "    \n",
    "    Arrangement: Upper rows = superior slices (toward head)\n",
    "                 Lower rows = inferior slices (toward feet)\n",
    "    \"\"\"\n",
    "    # Load patient data\n",
    "    filepath = test_files[patient_idx]\n",
    "    data = np.load(filepath)\n",
    "    patches = data['patches'].astype(np.float32) / 255.0\n",
    "    segs = data['segmentations']\n",
    "    \n",
    "    # Find patch with tumor (for more interesting visualization)\n",
    "    patch_idx = 0\n",
    "    max_tumor = 0\n",
    "    for i in range(len(segs)):\n",
    "        tumor_count = np.sum(segs[i] == 2)\n",
    "        if tumor_count > max_tumor:\n",
    "            max_tumor = tumor_count\n",
    "            patch_idx = i\n",
    "    \n",
    "    patch = patches[patch_idx]\n",
    "    gt = segs[patch_idx]\n",
    "    \n",
    "    # Predict\n",
    "    x = patch[np.newaxis, ..., np.newaxis]\n",
    "    pred = model.predict(x, verbose=0)\n",
    "    pred_mask = np.argmax(pred[0], axis=-1)\n",
    "    \n",
    "    # Get slice indices evenly distributed through the volume\n",
    "    # In CT: higher slice index = superior (head), lower = inferior (feet)\n",
    "    depth = patch.shape[0]\n",
    "    slice_indices = np.linspace(depth - 1, 0, num_slices, dtype=int)  # Superior to inferior\n",
    "    \n",
    "    # Create figure\n",
    "    fig, axes = plt.subplots(num_slices, 3, figsize=(10, 3 * num_slices))\n",
    "    \n",
    "    # Column titles\n",
    "    col_titles = ['Original Patch', 'Ground Truth', 'Predicted Mask']\n",
    "    \n",
    "    # Color map for segmentation\n",
    "    seg_cmap = plt.cm.colors.ListedColormap(['black', 'green', 'red'])\n",
    "    \n",
    "    for row, s in enumerate(slice_indices):\n",
    "        # Plot original patch\n",
    "        axes[row, 0].imshow(patch[s], cmap='gray', vmin=0, vmax=1)\n",
    "        axes[row, 0].set_ylabel(f'Slice {s}\\n({\"Superior\" if row < num_slices//2 else \"Inferior\"})', \n",
    "                                 fontsize=9)\n",
    "        axes[row, 0].set_xticks([])\n",
    "        axes[row, 0].set_yticks([])\n",
    "        \n",
    "        # Plot ground truth overlay\n",
    "        axes[row, 1].imshow(patch[s], cmap='gray', vmin=0, vmax=1, alpha=0.5)\n",
    "        axes[row, 1].imshow(gt[s], cmap=seg_cmap, vmin=0, vmax=2, alpha=0.5)\n",
    "        axes[row, 1].set_xticks([])\n",
    "        axes[row, 1].set_yticks([])\n",
    "        \n",
    "        # Plot prediction overlay\n",
    "        axes[row, 2].imshow(patch[s], cmap='gray', vmin=0, vmax=1, alpha=0.5)\n",
    "        axes[row, 2].imshow(pred_mask[s], cmap=seg_cmap, vmin=0, vmax=2, alpha=0.5)\n",
    "        axes[row, 2].set_xticks([])\n",
    "        axes[row, 2].set_yticks([])\n",
    "        \n",
    "        # Add column titles on first row\n",
    "        if row == 0:\n",
    "            for col, title in enumerate(col_titles):\n",
    "                axes[row, col].set_title(title, fontsize=12, fontweight='bold')\n",
    "    \n",
    "    # Add legend\n",
    "    from matplotlib.patches import Patch\n",
    "    legend_elements = [\n",
    "        Patch(facecolor='black', label='Background'),\n",
    "        Patch(facecolor='green', label='Liver'),\n",
    "        Patch(facecolor='red', label='Tumor')\n",
    "    ]\n",
    "    fig.legend(handles=legend_elements, loc='lower center', ncol=3, fontsize=10,\n",
    "               bbox_to_anchor=(0.5, -0.02))\n",
    "    \n",
    "    # Add main title\n",
    "    patient_name = os.path.basename(filepath)\n",
    "    fig.suptitle(f'Patient: {patient_name}\\nPatch {patch_idx} (Tumor voxels: {max_tumor:,})\\n↑ Superior (Head)  |  ↓ Inferior (Feet)', \n",
    "                 fontsize=12, fontweight='bold', y=1.02)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(bottom=0.05, top=0.93)\n",
    "    plt.savefig('single_patient_slices.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Visualization saved to single_patient_slices.png\")\n",
    "    print(f\"Showing {num_slices} slices from depth 0-{depth-1}\")\n",
    "\n",
    "\n",
    "# Run visualization for first test patient\n",
    "print(\"Generating slice visualization for single patient...\")\n",
    "visualize_single_patient_slices(model, test_files, patient_idx=2, num_slices=8)"
   ]
  },
  {
   "cell_type": "code",
   "source": "# =============================================================================\n# 3D Visualization: Isosurface Rendering of Liver and Tumor\n# =============================================================================\n\nfrom skimage import measure\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom mpl_toolkits.mplot3d.art3d import Poly3DCollection\n\ndef visualize_3d_segmentation(model, test_files, patient_idx=0, downsample=2):\n    \"\"\"\n    Visualize 3D isosurface rendering of liver and tumor segmentation.\n    \n    Parameters:\n    - model: Trained model\n    - test_files: List of test file paths\n    - patient_idx: Which patient to visualize\n    - downsample: Factor to downsample volume for faster rendering (default=2)\n    \"\"\"\n    # Load patient data\n    filepath = test_files[patient_idx]\n    data = np.load(filepath)\n    patches = data['patches'].astype(np.float32) / 255.0\n    segs = data['segmentations']\n    \n    # Find patch with most tumor content\n    patch_idx = 0\n    max_tumor = 0\n    for i in range(len(segs)):\n        tumor_count = np.sum(segs[i] == 2)\n        if tumor_count > max_tumor:\n            max_tumor = tumor_count\n            patch_idx = i\n    \n    patch = patches[patch_idx]\n    gt = segs[patch_idx]\n    \n    # Predict\n    x = patch[np.newaxis, ..., np.newaxis]\n    pred = model.predict(x, verbose=0)\n    pred_mask = np.argmax(pred[0], axis=-1)\n    \n    # Downsample for faster rendering\n    gt_ds = gt[::downsample, ::downsample, ::downsample]\n    pred_ds = pred_mask[::downsample, ::downsample, ::downsample]\n    \n    # Create figure with 2 subplots (GT and Prediction)\n    fig = plt.figure(figsize=(16, 7))\n    \n    titles = ['Ground Truth', 'Prediction']\n    volumes = [gt_ds, pred_ds]\n    \n    for idx, (title, vol) in enumerate(zip(titles, volumes)):\n        ax = fig.add_subplot(1, 2, idx + 1, projection='3d')\n        \n        # Extract and plot liver surface (class 1)\n        liver_mask = (vol >= 1).astype(np.float32)  # Liver includes tumor region\n        if liver_mask.sum() > 0:\n            try:\n                verts, faces, _, _ = measure.marching_cubes(liver_mask, level=0.5)\n                mesh = Poly3DCollection(verts[faces], alpha=0.3, linewidths=0)\n                mesh.set_facecolor('green')\n                mesh.set_edgecolor('darkgreen')\n                ax.add_collection3d(mesh)\n            except:\n                pass  # Skip if no surface found\n        \n        # Extract and plot tumor surface (class 2)\n        tumor_mask = (vol == 2).astype(np.float32)\n        if tumor_mask.sum() > 0:\n            try:\n                verts, faces, _, _ = measure.marching_cubes(tumor_mask, level=0.5)\n                mesh = Poly3DCollection(verts[faces], alpha=0.9, linewidths=0)\n                mesh.set_facecolor('red')\n                mesh.set_edgecolor('darkred')\n                ax.add_collection3d(mesh)\n            except:\n                pass  # Skip if no surface found\n        \n        # Set axis properties\n        ax.set_xlim(0, vol.shape[0])\n        ax.set_ylim(0, vol.shape[1])\n        ax.set_zlim(0, vol.shape[2])\n        ax.set_xlabel('X (Depth)', fontsize=10)\n        ax.set_ylabel('Y (Height)', fontsize=10)\n        ax.set_zlabel('Z (Width)', fontsize=10)\n        ax.set_title(f'{title}\\nLiver (green), Tumor (red)', fontsize=12, fontweight='bold')\n        \n        # Set viewing angle\n        ax.view_init(elev=20, azim=45)\n    \n    # Add main title\n    patient_name = os.path.basename(filepath)\n    fig.suptitle(f'3D Segmentation Visualization\\nPatient: {patient_name}, Patch {patch_idx}\\n'\n                 f'Tumor voxels: {max_tumor:,} | Downsampled {downsample}x for rendering',\n                 fontsize=12, fontweight='bold', y=1.02)\n    \n    plt.tight_layout()\n    plt.savefig('3d_visualization.png', dpi=150, bbox_inches='tight')\n    plt.show()\n    \n    print(f\"3D visualization saved to 3d_visualization.png\")\n    print(f\"Original volume: {gt.shape}, Downsampled: {gt_ds.shape}\")\n\n\n# Run 3D visualization\nprint(\"Generating 3D isosurface visualization...\")\nvisualize_3d_segmentation(model, test_files, patient_idx=2, downsample=2)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# =============================================================================\n# 3D Visualization: Multi-View Rendering (6 angles)\n# =============================================================================\n\ndef visualize_3d_multiview(model, test_files, patient_idx=0, downsample=2):\n    \"\"\"\n    Visualize 3D segmentation from 6 different viewing angles.\n    Shows Ground Truth vs Prediction side by side for each angle.\n    \n    Parameters:\n    - model: Trained model\n    - test_files: List of test file paths\n    - patient_idx: Which patient to visualize\n    - downsample: Factor to downsample volume for faster rendering\n    \"\"\"\n    from skimage import measure\n    from mpl_toolkits.mplot3d.art3d import Poly3DCollection\n    \n    # Load patient data\n    filepath = test_files[patient_idx]\n    data = np.load(filepath)\n    patches = data['patches'].astype(np.float32) / 255.0\n    segs = data['segmentations']\n    \n    # Find patch with most tumor content\n    patch_idx = 0\n    max_tumor = 0\n    for i in range(len(segs)):\n        tumor_count = np.sum(segs[i] == 2)\n        if tumor_count > max_tumor:\n            max_tumor = tumor_count\n            patch_idx = i\n    \n    patch = patches[patch_idx]\n    gt = segs[patch_idx]\n    \n    # Predict\n    x = patch[np.newaxis, ..., np.newaxis]\n    pred = model.predict(x, verbose=0)\n    pred_mask = np.argmax(pred[0], axis=-1)\n    \n    # Downsample for faster rendering\n    gt_ds = gt[::downsample, ::downsample, ::downsample]\n    pred_ds = pred_mask[::downsample, ::downsample, ::downsample]\n    \n    # Define viewing angles: (elevation, azimuth, name)\n    views = [\n        (20, 45, 'Front-Right'),\n        (20, 135, 'Front-Left'),\n        (20, 225, 'Back-Left'),\n        (20, 315, 'Back-Right'),\n        (90, 0, 'Top (Axial)'),\n        (0, 0, 'Front (Coronal)'),\n    ]\n    \n    # Create figure: 6 rows (views) x 2 columns (GT, Pred)\n    fig = plt.figure(figsize=(10, 24))\n    \n    volumes = [('Ground Truth', gt_ds), ('Prediction', pred_ds)]\n    \n    for view_idx, (elev, azim, view_name) in enumerate(views):\n        for col_idx, (title, vol) in enumerate(volumes):\n            ax = fig.add_subplot(6, 2, view_idx * 2 + col_idx + 1, projection='3d')\n            \n            # Extract and plot liver surface\n            liver_mask = (vol >= 1).astype(np.float32)\n            if liver_mask.sum() > 0:\n                try:\n                    verts, faces, _, _ = measure.marching_cubes(liver_mask, level=0.5)\n                    mesh = Poly3DCollection(verts[faces], alpha=0.3, linewidths=0)\n                    mesh.set_facecolor('green')\n                    ax.add_collection3d(mesh)\n                except:\n                    pass\n            \n            # Extract and plot tumor surface\n            tumor_mask = (vol == 2).astype(np.float32)\n            if tumor_mask.sum() > 0:\n                try:\n                    verts, faces, _, _ = measure.marching_cubes(tumor_mask, level=0.5)\n                    mesh = Poly3DCollection(verts[faces], alpha=0.9, linewidths=0)\n                    mesh.set_facecolor('red')\n                    ax.add_collection3d(mesh)\n                except:\n                    pass\n            \n            # Set axis properties\n            ax.set_xlim(0, vol.shape[0])\n            ax.set_ylim(0, vol.shape[1])\n            ax.set_zlim(0, vol.shape[2])\n            ax.set_xlabel('X', fontsize=8)\n            ax.set_ylabel('Y', fontsize=8)\n            ax.set_zlabel('Z', fontsize=8)\n            ax.view_init(elev=elev, azim=azim)\n            \n            # Title for top row only\n            if view_idx == 0:\n                ax.set_title(title, fontsize=11, fontweight='bold')\n            \n            # View name for first column only\n            if col_idx == 0:\n                ax.text2D(-0.15, 0.5, view_name, transform=ax.transAxes, \n                         fontsize=10, fontweight='bold', rotation=90,\n                         ha='center', va='center')\n    \n    # Add main title\n    patient_name = os.path.basename(filepath)\n    fig.suptitle(f'3D Multi-View Segmentation\\nPatient: {patient_name}, Patch {patch_idx}\\n'\n                 f'Green=Liver, Red=Tumor | Tumor voxels: {max_tumor:,}',\n                 fontsize=12, fontweight='bold', y=0.995)\n    \n    plt.tight_layout()\n    plt.subplots_adjust(top=0.94, left=0.12)\n    plt.savefig('3d_multiview.png', dpi=150, bbox_inches='tight')\n    plt.show()\n    \n    print(f\"Multi-view 3D visualization saved to 3d_multiview.png\")\n\n\n# Run multi-view 3D visualization\nprint(\"Generating multi-view 3D visualization...\")\nvisualize_3d_multiview(model, test_files, patient_idx=2, downsample=2)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}