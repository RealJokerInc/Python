{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RBC Classification (Grayscale)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import visualkeras\n",
    "#import cv2\n",
    "from tensorflow.keras import layers, models, regularizers\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from PIL import Image, ImageOps, ImageFilter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Data preparation parameters\n",
    "DATASET_DIR = \"bloodcells_dataset\"  \n",
    "IMG_SIZE = 100  # Downsize size before cropping\n",
    "CROP_FACTOR = 0.6  \n",
    "CROPPED_SIZE = int(IMG_SIZE * CROP_FACTOR)  # Final size after cropping (120x120 with 0.6 factor)\n",
    "VAL_FRAC = 0.15\n",
    "TEST_FRAC = 0.15\n",
    "LIMIT_PER_CLASS = None  # Set to a number to limit images per class\n",
    "SEGMENTATION_THRESHOLD = 0  # Pixels ABOVE this will be set to zero (inverted)\n",
    "SEGMENTATION_THRESHOLD_UPPER = 140 # Pixels ABOVE this will be set to zero (inverted)\n",
    "\n",
    "# Training parameters\n",
    "EPOCHS = 12\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Output paths\n",
    "OUT_DIR = \"./artifacts_grey\"\n",
    "OUT_NPZ = os.path.join(OUT_DIR, \"rbc_data_grey.npz\")\n",
    "\n",
    "# Create output directories\n",
    "for d in [OUT_DIR, f\"{OUT_DIR}/models\", f\"{OUT_DIR}/plots\", f\"{OUT_DIR}/reports\"]:\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "\n",
    "print(f\"Dataset directory: {os.path.abspath(DATASET_DIR)}\")\n",
    "print(f\"Output directory: {os.path.abspath(OUT_DIR)}\")\n",
    "print(f\"Original image size: {IMG_SIZE}x{IMG_SIZE}\")\n",
    "print(f\"Crop factor: {CROP_FACTOR}\")\n",
    "print(f\"Final cropped size: {CROPPED_SIZE}x{CROPPED_SIZE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "def get_class_names(dataset_dir: str) -> List[str]:\n",
    "    \"\"\"Get sorted list of class names from dataset directory.\"\"\"\n",
    "    return sorted([d.name for d in Path(dataset_dir).iterdir() if d.is_dir()])\n",
    "\n",
    "def center_crop(im: Image.Image, crop_factor: float = CROP_FACTOR) -> Image.Image:\n",
    "    \"\"\"Apply center crop based on crop_factor (0 < crop_factor <= 1).\"\"\"\n",
    "    if crop_factor <= 0 or crop_factor > 1:\n",
    "        raise ValueError(\"crop_factor must be between 0 and 1\")\n",
    "    \n",
    "    width, height = im.size\n",
    "    new_width = int(width * crop_factor)\n",
    "    new_height = int(height * crop_factor)\n",
    "    \n",
    "    left = (width - new_width) // 2\n",
    "    top = (height - new_height) // 2\n",
    "    right = left + new_width\n",
    "    bottom = top + new_height\n",
    "    \n",
    "    return im.crop((left, top, right, bottom))\n",
    "\n",
    "def segment_image_inverted(im: Image.Image, threshold: float = SEGMENTATION_THRESHOLD, threshold_upper: float = SEGMENTATION_THRESHOLD_UPPER) -> Image.Image:\n",
    "    \"\"\"Apply INVERTED segmentation using PIL by setting pixel values ABOVE threshold to zero.\"\"\"\n",
    "    im_gray = im.convert('L')\n",
    "    arr = np.asarray(im_gray, dtype=np.float32)\n",
    "    \n",
    "    # Apply inverted segmentation: pixels ABOVE threshold -> 0\n",
    "    arr[arr <= threshold] = 0.0\n",
    "    arr[arr >= threshold_upper] = 0.0\n",
    "    return Image.fromarray(arr.astype(np.uint8), mode='L')\n",
    "\n",
    "def process_image(im: Image.Image) -> np.ndarray:\n",
    "    \"\"\"Apply image processing steps to a PIL Image - GRAYSCALE VERSION with INVERSE SEGMENTATION FIRST and Sobel filter channel.\"\"\"\n",
    "    # Step 1: Segmentation\n",
    "    im = segment_image_inverted(im, threshold=SEGMENTATION_THRESHOLD, threshold_upper=SEGMENTATION_THRESHOLD_UPPER)\n",
    "    # Step 2: Center crop\n",
    "    im = center_crop(im, crop_factor=CROP_FACTOR)\n",
    "    # Step 3: Resize\n",
    "    im = im.resize((CROPPED_SIZE, CROPPED_SIZE))\n",
    "    # Step 4: (Optional median filter - currently commented out)\n",
    "    #im = im.filter(ImageFilter.MedianFilter(size=3))\n",
    "    \n",
    "    # Convert to numpy array and normalize for grayscale channel\n",
    "    arr_gray = np.asarray(im, dtype=np.float32) / 255.0\n",
    "    \n",
    "    # Apply Sobel filter after step 4\n",
    "    # Sobel X and Y gradients\n",
    "    sobel_x = np.array([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]], dtype=np.float32)\n",
    "    sobel_y = np.array([[-1, -2, -1], [0, 0, 0], [1, 2, 1]], dtype=np.float32)\n",
    "    \n",
    "    from scipy.ndimage import convolve\n",
    "    grad_x = convolve(arr_gray, sobel_x)\n",
    "    grad_y = convolve(arr_gray, sobel_y)\n",
    "    \n",
    "    # Compute gradient magnitude\n",
    "    sobel_magnitude = np.sqrt(grad_x**2 + grad_y**2)\n",
    "    \n",
    "    # Normalize Sobel magnitude to [0, 1]\n",
    "    sobel_magnitude = sobel_magnitude / (sobel_magnitude.max() + 1e-8)\n",
    "    \n",
    "    # Stack grayscale and Sobel channels\n",
    "    arr_combined = np.stack([arr_gray, sobel_magnitude], axis=-1)\n",
    "    \n",
    "    return arr_combined\n",
    "\n",
    "def load_images(dataset_dir: str, class_names: List[str]):\n",
    "    \"\"\"Load all JPG images from dataset directory.\"\"\"\n",
    "    X_list, y_list = [], []\n",
    "    dataset_path = Path(dataset_dir)\n",
    "    \n",
    "    for cls_idx, cls_name in enumerate(class_names, start=1):\n",
    "        # Use glob to get all jpg files\n",
    "        jpg_files = list((dataset_path / cls_name).glob('*.jpg'))\n",
    "        \n",
    "        print(f\"Loading {len(jpg_files)} images from {cls_name}...\")\n",
    "        \n",
    "        for img_path in jpg_files:\n",
    "            with Image.open(img_path) as im:\n",
    "                arr = process_image(im)\n",
    "            X_list.append(arr)\n",
    "            y_list.append(cls_idx)\n",
    "    \n",
    "    return np.stack(X_list, axis=0), np.array(y_list, dtype=np.int64)\n",
    "\n",
    "def split_data(X, y):\n",
    "    \"\"\"Split data into train, validation, and test sets.\"\"\"\n",
    "    # First split: train vs (val + test)\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "        X, y, \n",
    "        test_size=(VAL_FRAC + TEST_FRAC), \n",
    "        stratify=y, \n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Second split: val vs test\n",
    "    val_ratio = VAL_FRAC / (VAL_FRAC + TEST_FRAC)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(\n",
    "        X_temp, y_temp, \n",
    "        test_size=(1 - val_ratio), \n",
    "        stratify=y_temp, \n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "print(\"Data preparation functions loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Get class names\n",
    "class_names = get_class_names(DATASET_DIR)\n",
    "print(f\"Classes found: {class_names}\")\n",
    "num_classes = len(class_names)\n",
    "\n",
    "# Load images first using the processing pipeline\n",
    "print(\"\\nLoading images for augmentation...\")\n",
    "X_orig, y_orig = load_images(DATASET_DIR, class_names)\n",
    "print(f\"Loaded: X shape = {X_orig.shape}, y shape = {y_orig.shape}\")\n",
    "\n",
    "def augment_image(img_2channel: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Apply random translation and rotation to a 2-channel image.\"\"\"\n",
    "    from scipy.ndimage import rotate, shift\n",
    "    \n",
    "    # Random translation offset (-10% to +10% of image size)\n",
    "    translate_x = (np.random.random() - 0.5) * 0.2 * CROPPED_SIZE\n",
    "    translate_y = (np.random.random() - 0.5) * 0.2 * CROPPED_SIZE\n",
    "    \n",
    "    # Random rotation angle (-30 to +30 degrees)\n",
    "    angle = (np.random.random() - 0.5) * 60\n",
    "    \n",
    "    # Apply transformations to both channels\n",
    "    augmented = np.zeros_like(img_2channel)\n",
    "    for ch in range(img_2channel.shape[-1]):\n",
    "        # Shift\n",
    "        shifted = shift(img_2channel[:, :, ch], [translate_y, translate_x], mode='nearest')\n",
    "        # Rotate\n",
    "        augmented[:, :, ch] = rotate(shifted, angle, reshape=False, mode='nearest')\n",
    "    \n",
    "    return augmented\n",
    "\n",
    "# Perform data augmentation, generating 1 augmented copy of each image\n",
    "print(\"\\nApplying data augmentation...\")\n",
    "X_augmented = np.zeros_like(X_orig)\n",
    "y_augmented = np.copy(y_orig)\n",
    "\n",
    "for i in range(len(X_orig)):\n",
    "    X_augmented[i] = augment_image(X_orig[i])\n",
    "\n",
    "# Combine original and augmented data\n",
    "X = np.concatenate((X_orig, X_augmented), axis=0)\n",
    "y = np.concatenate((y_orig, y_augmented), axis=0)\n",
    "\n",
    "print(f\"After augmentation: X shape = {X.shape}, y shape = {y.shape}\")\n",
    "\n",
    "# Split data\n",
    "print(\"\\nSplitting data...\")\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = split_data(X, y)\n",
    "print(f\"Train: {X_train.shape}\")\n",
    "print(f\"Val:   {X_val.shape}\")\n",
    "print(f\"Test:  {X_test.shape}\")\n",
    "\n",
    "# Save combined original + augmented data to the same OUT_NPZ file\n",
    "meta = {\n",
    "    \"class_names\": class_names,\n",
    "    \"original_img_size\": IMG_SIZE,\n",
    "    \"crop_factor\": CROP_FACTOR,\n",
    "    \"final_size\": CROPPED_SIZE,\n",
    "    \"grayscale\": True,\n",
    "    \"channels\": 2,\n",
    "    \"augmented\": True\n",
    "}\n",
    "np.savez_compressed(\n",
    "    OUT_NPZ,\n",
    "    X_train=X_train, y_train=y_train,\n",
    "    X_val=X_val, y_val=y_val,\n",
    "    X_test=X_test, y_test=y_test,\n",
    "    meta=json.dumps(meta)\n",
    ")\n",
    "print(f\"\\nSaved augmented data (original + augmented) to: {os.path.abspath(OUT_NPZ)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Processing Output \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "def visualize_processing_pipeline(dataset_dir, class_names, num_samples=3):\n",
    "    \"\"\"Visualize the image processing pipeline: Original RGB → Inverted Segmentation → Center Crop → Resize → Sobel Filter.\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(num_samples, 5, figsize=(20, 4*num_samples))\n",
    "    dataset_path = Path(dataset_dir)\n",
    "    \n",
    "    sample_count = 0\n",
    "    for cls_name in class_names:\n",
    "        if sample_count >= num_samples:\n",
    "            break\n",
    "        \n",
    "        # Use glob to get first jpg file\n",
    "        jpg_files = list((dataset_path / cls_name).glob('*.jpg'))\n",
    "        if not jpg_files:\n",
    "            continue\n",
    "        \n",
    "        img_path = jpg_files[0]\n",
    "        \n",
    "        # Step 1: Original RGB\n",
    "        im_rgb = Image.open(img_path).convert(\"RGB\")\n",
    "        arr_rgb = np.asarray(im_rgb, dtype=np.float32) / 255.0\n",
    "        \n",
    "        # Step 2: Apply inverted segmentation FIRST\n",
    "        im_segmented = segment_image_inverted(im_rgb, threshold=SEGMENTATION_THRESHOLD, threshold_upper=SEGMENTATION_THRESHOLD_UPPER)\n",
    "        im_segmented_display = im_segmented.resize((CROPPED_SIZE, CROPPED_SIZE))\n",
    "        arr_segmented = np.asarray(im_segmented_display, dtype=np.float32) / 255.0\n",
    "        \n",
    "        # Step 3: Center Crop (on segmented image)\n",
    "        im_cropped = center_crop(im_segmented, crop_factor=CROP_FACTOR)\n",
    "        im_cropped_display = im_cropped.resize((CROPPED_SIZE, CROPPED_SIZE))\n",
    "        arr_cropped = np.asarray(im_cropped_display, dtype=np.float32) / 255.0\n",
    "        \n",
    "        # Step 4: Final processed version (this is what the model sees) - returns 2 channels\n",
    "        arr_processed = process_image(im_rgb)  # Shape: (CROPPED_SIZE, CROPPED_SIZE, 2)\n",
    "        arr_final = arr_processed[:, :, 0]  # Grayscale channel\n",
    "        arr_sobel = arr_processed[:, :, 1]  # Sobel channel\n",
    "        \n",
    "        # Plot\n",
    "        axes[sample_count, 0].imshow(arr_rgb)\n",
    "        axes[sample_count, 0].set_title(f\"{cls_name}\\nOriginal RGB\")\n",
    "        axes[sample_count, 0].axis('off')\n",
    "        \n",
    "        axes[sample_count, 1].imshow(arr_segmented, cmap='gray')\n",
    "        axes[sample_count, 1].set_title(f\"Inverted Segmentation + Downsampling + Grayscale\\n(threshold=[{SEGMENTATION_THRESHOLD} {SEGMENTATION_THRESHOLD_UPPER}])\")\n",
    "        axes[sample_count, 1].axis('off')\n",
    "        \n",
    "        axes[sample_count, 2].imshow(arr_cropped, cmap='gray')\n",
    "        axes[sample_count, 2].set_title(f\"Center Crop\\n(factor={CROP_FACTOR})\")\n",
    "        axes[sample_count, 2].axis('off')\n",
    "        \n",
    "        axes[sample_count, 3].imshow(arr_final, cmap='gray')\n",
    "        axes[sample_count, 3].set_title(f\"Final Grayscale Channel\\n(Model Input)\")\n",
    "        axes[sample_count, 3].axis('off')\n",
    "        \n",
    "        axes[sample_count, 4].imshow(arr_sobel, cmap='gray')\n",
    "        axes[sample_count, 4].set_title(f\"Final Sobel Edge Channel\\n(Model Input)\")\n",
    "        axes[sample_count, 4].axis('off')\n",
    "        \n",
    "        im_rgb.close()\n",
    "        sample_count += 1\n",
    "    \n",
    "    plt.suptitle(f'Image Processing Pipeline (Resize → Segmentation → Crop → Sobel) - Final: {CROPPED_SIZE}x{CROPPED_SIZE}x2', fontsize=16, y=1.00)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{OUT_DIR}/plots/processing_pipeline.png\", dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f\" Processing pipeline visualization saved to: {OUT_DIR}/plots/processing_pipeline.png\")\n",
    "\n",
    "# Visualize processing for 3 sample images\n",
    "visualize_processing_pipeline(DATASET_DIR, class_names, num_samples=min(3, num_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Display count of images per class in the dataset\n",
    "print(\"=\"*60)\n",
    "print(\"IMAGE COUNT PER CLASS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "dataset_path = Path(DATASET_DIR)\n",
    "class_counts = {}\n",
    "\n",
    "for cls_name in class_names:\n",
    "    class_folder = dataset_path / cls_name\n",
    "    # Count jpg files in each class folder\n",
    "    jpg_files = list(class_folder.glob('*.jpg'))\n",
    "    class_counts[cls_name] = len(jpg_files)\n",
    "\n",
    "# Display as table\n",
    "print(f\"{'Class Name':<20} {'Image Count':>15}\")\n",
    "print(\"-\"*60)\n",
    "total_images = 0\n",
    "for cls_name, count in class_counts.items():\n",
    "    print(f\"{cls_name:<20} {count:>15}\")\n",
    "    total_images += count\n",
    "\n",
    "print(\"-\"*60)\n",
    "print(f\"{'TOTAL':<20} {total_images:>15}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Optional: Display as bar chart\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(class_counts.keys(), class_counts.values(), color='steelblue', alpha=0.8, edgecolor='black')\n",
    "plt.xlabel('Class Name')\n",
    "plt.ylabel('Number of Images')\n",
    "plt.title('Image Distribution Across Classes')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{OUT_DIR}/plots/class_distribution.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"\\nClass distribution plot saved to: {OUT_DIR}/plots/class_distribution.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Display sample images from each class in a 2x4 grid\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "dataset_path = Path(DATASET_DIR)\n",
    "\n",
    "for idx, cls_name in enumerate(class_names):\n",
    "    if idx >= 8:  # Only show 8 classes\n",
    "        break\n",
    "    \n",
    "    # Get first image from this class\n",
    "    class_folder = dataset_path / cls_name\n",
    "    jpg_files = list(class_folder.glob('*.jpg'))\n",
    "    \n",
    "    if jpg_files:\n",
    "        # Load and display the image\n",
    "        img = Image.open(jpg_files[0])\n",
    "        img_array = np.array(img)\n",
    "        \n",
    "        # Display the image\n",
    "        if len(img_array.shape) == 3:  # Color image\n",
    "            axes[idx].imshow(img_array)\n",
    "        else:  # Grayscale\n",
    "            axes[idx].imshow(img_array, cmap='gray')\n",
    "        \n",
    "        # Set title with class name and dimensions\n",
    "        height, width = img_array.shape[:2]\n",
    "        axes[idx].set_title(f\"{cls_name}\\n{width} x {height}\", fontsize=12)\n",
    "        axes[idx].axis('off')\n",
    "    else:\n",
    "        axes[idx].text(0.5, 0.5, 'No image found', ha='center', va='center')\n",
    "        axes[idx].set_title(cls_name, fontsize=12)\n",
    "        axes[idx].axis('off')\n",
    "\n",
    "# Hide any unused subplots if less than 8 classes\n",
    "for idx in range(len(class_names), 8):\n",
    "    axes[idx].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{OUT_DIR}/plots/sample_images_per_class.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"\\nSample images grid saved to: {OUT_DIR}/plots/sample_images_per_class.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Model Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "def build_cnn(input_shape, num_classes,dropout_rate=0.5):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    x = inputs\n",
    "\n",
    "    # Block 1\n",
    "    x = layers.Conv2D(16, 3, padding=\"same\")(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.MaxPool2D()(x)\n",
    "    #x = layers.Dropout(dropout_rate)(x)\n",
    "\n",
    "    # Block 2\n",
    "    x = layers.Conv2D(32, 3, padding=\"same\")(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.MaxPool2D()(x)\n",
    "    #x = layers.Dropout(dropout_rate)(x)\n",
    "\n",
    "    # Block 2\n",
    "    x = layers.Conv2D(16, 3, padding=\"same\")(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.MaxPool2D()(x)\n",
    "    #x = layers.Dropout(dropout_rate)(x)\n",
    "\n",
    "    # Block 3\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(96, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "    x = layers.Dense(96, activation=\"relu\")(x)\n",
    "    outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "    return models.Model(inputs, outputs, name=\"tiny_cnn_regularized\")\n",
    "   \n",
    "\n",
    "\n",
    "def build_big_cnn(input_shape, num_classes, dropout_rate=0.3):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    x = inputs\n",
    "\n",
    "    # Block 1\n",
    "    x = layers.Conv2D(32, 3, padding=\"same\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.Conv2D(32, 3, padding=\"same\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.MaxPool2D()(x)\n",
    "    x = layers.Dropout(0.25)(x)\n",
    "\n",
    "    # Block 2\n",
    "    x = layers.Conv2D(64, 3, padding=\"same\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.Conv2D(64, 3, padding=\"same\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.MaxPool2D()(x)\n",
    "    x = layers.Dropout(0.25)(x)\n",
    "\n",
    "    # Block 3\n",
    "    x = layers.Conv2D(128, 3, padding=\"same\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.Conv2D(128, 3, padding=\"same\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.MaxPool2D()(x)\n",
    "    x = layers.Dropout(0.25)(x)\n",
    "\n",
    "    # Dense layers\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(256, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "    x = layers.Dense(128, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "    outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "    return tf.keras.Model(inputs, outputs, name=\"improved_cnn\")\n",
    "\n",
    "def build_logreg_baseline(input_shape, num_classes):\n",
    "    \"\"\"Build a logistic regression baseline.\"\"\"\n",
    "    return tf.keras.Sequential([\n",
    "        layers.Input(shape=input_shape),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(num_classes, activation=\"sigmoid\")\n",
    "    ])\n",
    "\n",
    "print(\"Model building functions loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "def train_model(model, X_train, y_train, X_val, y_val, model_name, epochs=EPOCHS, batch_size=BATCH_SIZE):\n",
    "    \"\"\"Train a model and plot training curves.\"\"\"\n",
    "    # Convert labels to categorical\n",
    "    y_train_cat = to_categorical(y_train - 1, num_classes=num_classes)\n",
    "    y_val_cat = to_categorical(y_val - 1, num_classes=num_classes)\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(\n",
    "        optimizer=\"adam\",\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    print(f\"\\nTraining {model_name}...\")\n",
    "    \n",
    "    history = model.fit(\n",
    "        X_train, y_train_cat,\n",
    "        validation_data=(X_val, y_val_cat),\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Plot accuracy\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history[\"accuracy\"], label=\"Train\")\n",
    "    plt.plot(history.history[\"val_accuracy\"], label=\"Validation\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.title(f\"{model_name} - Accuracy\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot loss\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history[\"loss\"], label=\"Train\")\n",
    "    plt.plot(history.history[\"val_loss\"], label=\"Validation\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(f\"{model_name} - Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{OUT_DIR}/plots/{model_name}_training.png\", dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    return history\n",
    "\n",
    "def evaluate_model(model, X_test, y_test, model_name):\n",
    "    \"\"\"Evaluate model and generate confusion matrix and classification report.\"\"\"\n",
    "    # Get predictions\n",
    "    y_true = (y_test - 1).astype(int)\n",
    "    y_pred = np.argmax(model.predict(X_test, verbose=0), axis=1)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    print(f\"\\n{model_name} Test Accuracy: {acc:.4f}\")\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=list(range(num_classes)))\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(cm, interpolation=\"nearest\", cmap=\"Blues\")\n",
    "    plt.title(f\"{model_name} - Confusion Matrix\")\n",
    "    plt.colorbar()\n",
    "    \n",
    "    ticks = np.arange(num_classes)\n",
    "    plt.xticks(ticks, class_names, rotation=45, ha=\"right\")\n",
    "    plt.yticks(ticks, class_names)\n",
    "    \n",
    "    # Add text annotations\n",
    "    for i in range(num_classes):\n",
    "        for j in range(num_classes):\n",
    "            plt.text(j, i, str(cm[i, j]), ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > cm.max() / 2 else \"black\")\n",
    "    \n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{OUT_DIR}/plots/{model_name}_cm.png\", dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Classification report\n",
    "    report = classification_report(y_true, y_pred, target_names=class_names, digits=4)\n",
    "    print(f\"\\n{model_name} Classification Report:\")\n",
    "    print(report)\n",
    "    \n",
    "    # Save report to file\n",
    "    with open(f\"{OUT_DIR}/reports/{model_name}_report.txt\", \"w\") as f:\n",
    "        f.write(f\"Test Accuracy: {acc:.4f}\\n\\n\")\n",
    "        f.write(report)\n",
    "    \n",
    "    return acc\n",
    "\n",
    "print(\"Training and evaluation functions loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "input_shape = (CROPPED_SIZE, CROPPED_SIZE, 2)\n",
    "dropout_rate = 0.3\n",
    "cnn_model = build_cnn(input_shape, num_classes,dropout_rate)\n",
    "cnn_model.summary()\n",
    "\n",
    "# Train CNN\n",
    "cnn_history = train_model(cnn_model, X_train, y_train, X_val, y_val, \"tiny_cnn\")\n",
    "\n",
    "cnn_model.save(f\"{OUT_DIR}/models/tiny_cnn.keras\")\n",
    "print(f\"\\nCNN model saved to: {OUT_DIR}/models/tiny_cnn.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "cnn_accuracy = evaluate_model(cnn_model, X_test, y_test, \"tiny_cnn\")\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "plot_model(cnn_model, \n",
    "               to_file=f\"{OUT_DIR}/plots/cnn_model_architecture.png\", \n",
    "               show_shapes=True, \n",
    "               show_layer_names=True,\n",
    "               rankdir='TB',  # Top to Bottom\n",
    "               expand_nested=True, \n",
    "               dpi=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Logistic Regression Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build logistic regression model\n",
    "logreg_model = build_logreg_baseline(input_shape, num_classes)\n",
    "logreg_model.summary()\n",
    "\n",
    "# Train logistic regression\n",
    "logreg_history = train_model(logreg_model, X_train, y_train, X_val, y_val, \"logreg\")\n",
    "\n",
    "# Save model\n",
    "logreg_model.save(f\"{OUT_DIR}/models/logreg.keras\")\n",
    "print(f\"\\nLogistic regression model saved to: {OUT_DIR}/models/logreg.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Logistic Regression Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_accuracy = evaluate_model(logreg_model, X_test, y_test, \"logreg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare model accuracies\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"MODEL COMPARISON\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Tiny CNN Test Accuracy:        {cnn_accuracy:.4f}\")\n",
    "print(f\"Logistic Regression Accuracy:  {logreg_accuracy:.4f}\")\n",
    "print(f\"Improvement:                   {(cnn_accuracy - logreg_accuracy):.4f}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Bar plot comparison\n",
    "plt.figure(figsize=(8, 5))\n",
    "models = ['Logistic Regression', 'Tiny CNN']\n",
    "accuracies = [logreg_accuracy, cnn_accuracy]\n",
    "colors = ['#FF6B6B', '#4ECDC4']\n",
    "\n",
    "bars = plt.bar(models, accuracies, color=colors, alpha=0.8, edgecolor='black')\n",
    "plt.ylabel('Test Accuracy')\n",
    "plt.title('Model Comparison (Grayscale + Inverted Segmentation)')\n",
    "plt.ylim(0, 1.0)\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, acc in zip(bars, accuracies):\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{acc:.4f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{OUT_DIR}/plots/model_comparison.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import visualkeras\n",
    "\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "def build_cnn(input_shape, num_classes,dropout_rate=0.5):\n",
    "\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    x = inputs\n",
    "\n",
    "    # Block 1\n",
    "    x = layers.Conv2D(16, 3, padding=\"same\")(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.MaxPool2D()(x)\n",
    "    #x = layers.Dropout(dropout_rate)(x)\n",
    "\n",
    "    # Block 2\n",
    "    x = layers.Conv2D(32, 3, padding=\"same\")(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.MaxPool2D()(x)\n",
    "    #x = layers.Dropout(dropout_rate)(x)\n",
    "\n",
    "    # Block 2\n",
    "    x = layers.Conv2D(16, 3, padding=\"same\")(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.MaxPool2D()(x)\n",
    "    #x = layers.Dropout(dropout_rate)(x)\n",
    "\n",
    "    # Block 3\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(96, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "    x = layers.Dense(96, activation=\"relu\")(x)\n",
    "    outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "    return models.Model(inputs, outputs, name=\"tiny_cnn_regularized\")\n",
    "\n",
    "input_shape = (CROPPED_SIZE, CROPPED_SIZE, 2)\n",
    "dropout_rate = 0.5\n",
    "cnn_model = build_cnn(input_shape, num_classes,dropout_rate)\n",
    "\n",
    "\n",
    "\n",
    "# Text summary\n",
    "cnn_model.summary()\n",
    "\n",
    "# Diagram image (needs pydot + graphviz)\n",
    "#plot_model(cnn_model, to_file=\"model_structure.png\", show_shapes=True, expand_nested=True, dpi=160)\n",
    "visualkeras.layered_view(cnn_model, legend=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
